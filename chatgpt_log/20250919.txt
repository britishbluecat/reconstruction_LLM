ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¸ã¨ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
ãƒãƒ£ãƒƒãƒˆå±¥æ­´

ã‚ãªãŸ:
https://github.com/britishbluecat/reconstruction_LLM/tree/dev

ä¸Šè¨˜ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€ã¾ãšã¯ã€chatgpt_logã®å†…å®¹ã‚’ç¢ºèªã—ã¦ã€ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚

ã“ã®äºŒä»¶ã§ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ã§ãã¾ã™ã€‚ 20250915.txt 20250915.md
ChatGPT:

æ€è€ƒæ™‚é–“: 48s
ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã—ãŸã€‚ã¾ãšã€ãƒªãƒã‚¸ãƒˆãƒªæ¦‚è¦ã¯æŠŠæ¡ã§ãã¦ã„ã¾ã™ï¼š

PoCã®ç›®çš„ï¼šç‰©ä»¶æƒ…å ±ã‹ã‚‰ã€Œå»ºã¦æ›¿ãˆï¼ãƒªãƒãƒ™ï¼è§£ä½“ï¼ˆæ›´åœ°å£²å´ï¼‰ã€ã®3æ¡ˆæ¯”è¼ƒã€æ³•ä»¤ãƒ»åˆ¶åº¦ä¸Šã®ç•™æ„ç‚¹ã€ææ¡ˆæ–‡ãƒ‰ãƒ©ãƒ•ãƒˆã€ç¾åœ°ç¢ºèªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚ä½¿ã„æ–¹ã¯ requirements.txt ã§ç’°å¢ƒæ§‹ç¯‰â†’src/prompt.py å®Ÿè¡Œã®æƒ³å®šã€‚ä»Šå¾Œã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹è‰¯ã‚„æ³•ä»¤ãƒ‡ãƒ¼ã‚¿ã®RAGçµ„è¾¼ã¿ã€Markdownâ†’PDFæ•´å½¢ãŒã‚¿ã‚¹ã‚¯ã«æŒ™ãŒã£ã¦ã„ã¾ã™ã€‚ 
GitHub

ä¸€æ–¹ã§ã”æŒ‡å®šã® chatgpt_log/20250915.txt ã¨ chatgpt_log/20250915.md ã¯ã€GitHubå´ã®è¡¨ç¤ºã‚¨ãƒ©ãƒ¼ã§å†…å®¹æœ¬æ–‡ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸï¼ˆchatgpt_log é…ä¸‹ã‚„å½“è©²ãƒ•ã‚¡ã‚¤ãƒ«URLã«ã¯åˆ°é”ã§ãã‚‹ã‚‚ã®ã®ã€æœ¬æ–‡ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã™ï¼‰ã€‚ 
GitHub
+2
GitHub
+2

ã™ãã«ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ã‚’å®Œäº†ã™ã‚‹ãŸã‚ã«
ãŠæ‰‹æ•°ã§ã™ãŒã€ä»¥ä¸‹ã©ã¡ã‚‰ã‹ã§å†…å®¹ã‚’å…±æœ‰ã—ã¦ã„ãŸã ã‘ã‚Œã°ã€ãã®å ´ã§è¦ç‚¹æŠ½å‡ºã¨æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŒ–ã¾ã§è¡Œã„ã¾ã™ã€‚

2ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ20250915.txt / 20250915.mdï¼‰ã®æœ¬æ–‡ã‚’ã“ã®ãƒãƒ£ãƒƒãƒˆã«è²¼ã‚Šä»˜ã‘

åŒå†…å®¹ã‚’Gistã‚„Pastebinç­‰ã®ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§å…¬é–‹ï¼ˆURLã‚’å…±æœ‰ï¼‰

è²¼ã‚Šä»˜ã‘å½¢å¼ã®ä¾‹ï¼š

--- 20250915.txt ---
(æœ¬æ–‡)

--- 20250915.md ---
(æœ¬æ–‡)
å±Šãæ¬¡ç¬¬ã€è¦ç´„ï¼ˆç›®çš„/å…¥åŠ›/å‡ºåŠ›/ä¿ç•™èª²é¡Œ/æ„æ€æ±ºå®šï¼‰â†’ã‚¿ã‚¹ã‚¯åˆ†è§£ï¼ˆä»Šæ—¥ã‚„ã‚‹ã“ã¨ãƒ»ãƒ–ãƒ©ãƒ³ãƒ/PRæ–¹é‡ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ï¼‰â†’ä¸è¶³æƒ…å ±ãƒªã‚¹ãƒˆã€ã¾ã§ä¸€æ°—ã«ä»•ä¸Šã’ã¾ã™ã€‚

ChatGPT:
ã¯ã„ã€ãã®é€šã‚Šã§ã™ã€‚
ã€Œä¸å‹•ç”£å±‹ã•ã‚“å‘ã‘ã®Excelå‡¦ç†ãƒ•ãƒ­ãƒ¼ã€ã‚’ã¾ã¨ã‚ã‚‹ã¨ã€ä»¥ä¸‹ã®å½¢ãŒãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«ãªã‚Šã¾ã™ã€‚

1. åˆ—ã®æ‰±ã„ï¼ˆç‰©ç†å â‡” è«–ç†åï¼‰
Excelå´ï¼ˆä¸å‹•ç”£å±‹ã•ã‚“åˆ©ç”¨æ™‚ï¼‰ï¼š

ä½æ‰€ / ç¯‰å¹´ / é¢ç© / ç¯‰é€  / çŠ¶æ…‹ / å¸Œæœ›ç”¨ï¼ˆè«–ç†åã€æ—¥æœ¬èªï¼‰ã‚’ä½¿ç”¨ã€‚

å†…éƒ¨å‡¦ç†ï¼ˆGPTã«æ¸¡ã™ã¨ãï¼‰ï¼š

location / building_age / floor_area / structure / current_condition / desired_useï¼ˆç‰©ç†åã€è‹±èªï¼‰ã«å¤‰æ›ã€‚

dictionary ã§ç›¸äº’å¤‰æ›ã—ã¦å¸åã€‚

2. å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆtkinter GUIã§Excelé¸æŠï¼‰
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒGUIã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã€‚

pandas ã§èª­ã¿è¾¼ã¿ â†’ è«–ç†å/ç‰©ç†åã‚’æ­£è¦åŒ–ã€‚

å„è¡Œã‚’GPTã¸æ¸¡ã—ã€7åˆ—ã‚’ç”Ÿæˆï¼š

æ¡ˆ

åˆæœŸè²»ç”¨ç›®å®‰

å·¥æœŸæ„Ÿ

ãƒ¡ãƒªãƒƒãƒˆ

ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

ä¸»ãªãƒªã‚¹ã‚¯

å‘ã„ã¦ã„ã‚‹æ–½ä¸»

å…ƒã®Excelã«7åˆ—ã‚’è¿½åŠ ã€‚

ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ filename_å‡¦ç†æ¸ˆã¿.xlsx ã¨ã—ã¦ä¿å­˜ã€‚

3. å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: tkinter

å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: openai, python-dotenv, pandas, openpyxl

requirements.txt ã«è¿½åŠ ï¼š

openai
python-dotenv
pandas
openpyxl
4. ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆï¼ˆdev branchã«è¿½åŠ æ¨å¥¨ï¼‰
src/
 â”œâ”€â”€ prompt.py      # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
 â”œâ”€â”€ advisor.py     # CLIç‰ˆï¼ˆæ—¢å­˜ï¼‰
 â””â”€â”€ excel_gui.py   # GUI + Excelå‡¦ç†ç‰ˆ â†æ–°è¦
5. å®Ÿè£…ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼ˆæŠœç²‹ï¼‰
# src/excel_gui.py
import tkinter as tk
from tkinter import filedialog, messagebox
import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI

LOGICAL_TO_PHYSICAL = {"ä½æ‰€":"location","ç¯‰å¹´":"building_age","é¢ç©":"floor_area",
                       "ç¯‰é€ ":"structure","çŠ¶æ…‹":"current_condition","å¸Œæœ›ç”¨":"desired_use"}

OUTPUT_COLS = ["æ¡ˆ","åˆæœŸè²»ç”¨ç›®å®‰","å·¥æœŸæ„Ÿ","ãƒ¡ãƒªãƒƒãƒˆ","ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ","ä¸»ãªãƒªã‚¹ã‚¯","å‘ã„ã¦ã„ã‚‹æ–½ä¸»"]

def main():
    root = tk.Tk(); root.withdraw()
    filepath = filedialog.askopenfilename(filetypes=[("Excel","*.xlsx")])
    if not filepath: return
    df = pd.read_excel(filepath)
    # è«–ç†åâ†’ç‰©ç†åå¤‰æ›
    df = df.rename(columns={j:p for j,p in LOGICAL_TO_PHYSICAL.items()})
    # å„è¡Œå‡¦ç†ï¼ˆGPTå‘¼ã³å‡ºã—éƒ¨åˆ†ã¯çœç•¥ï¼‰
    for idx,row in df.iterrows():
        df.loc[idx, OUTPUT_COLS] = ["æ¡ˆA","è²»ç”¨1000ä¸‡","6ãƒ¶æœˆ","æ–°ç¯‰åŒæ§˜","é«˜é¡","åœ°ç›¤ãƒªã‚¹ã‚¯","é•·æœŸå±…ä½è€…"]
    outpath = filepath.replace(".xlsx","_å‡¦ç†æ¸ˆã¿.xlsx")
    df.to_excel(outpath,index=False)
    messagebox.showinfo("å®Œäº†",f"ä¿å­˜ã—ã¾ã—ãŸ: {outpath}")

if __name__=="__main__":
    main()
âœ… ã“ã®å½¢ãªã‚‰ã€ä¸å‹•ç”£å±‹ã•ã‚“ãŒæ…£ã‚ŒãŸ Excel ã§å…¥åŠ›ãƒ»å‡ºåŠ›ã§ãã€å†…éƒ¨çš„ã«ã¯ GPT ã‚’å™›ã¾ã›ã¦7åˆ—ã‚’è‡ªå‹•ç”Ÿæˆã§ãã¾ã™ã€‚
ğŸ‘‰ ç¥å´ã•ã‚“ã€ã“ã® excel_gui.py ã‚’ dev branch ã«ç½®ã„ã¦è©¦ã—ã€ã¾ãšã¯ã€ŒGPTå‘¼ã³å‡ºã—ã‚’ãƒ€ãƒŸãƒ¼å€¤ã€ã§åŸ‹ã‚ã‚‹â†’æ¬¡ã«å®Ÿéš›ã® GPT å¿œç­”ã‚’çµ„ã¿è¾¼ã‚€ã€ã¨ã„ã†æ®µéšçš„é€²ã‚æ–¹ã§è‰¯ã„ã§ã™ã‹ï¼Ÿ

ã‚ãªãŸ:
ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ã§ãã¾ã—ãŸã‹ã€‚
ChatGPT:
ã¯ã„ã€ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—å®Œäº†ã—ã¾ã—ãŸ âœ…

ä»Šå›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã„ãŸã ã„ãŸ 20250915.txt / 20250915.md ã®å†…å®¹ã‚’æ•´ç†ã™ã‚‹ã¨ï¼š

ğŸ“Œ 20250915 ã®é–‹ç™ºãƒ­ã‚°ã®è¦ç‚¹
ä¿®æ­£å¯¾è±¡
src/storage.py

ä¿®æ­£å†…å®¹
ä¿å­˜ã™ã‚‹åˆ—ã‚’ 7åˆ—å›ºå®š ã«çµã‚‹ï¼š
url, fetched_at, status, location, station_walk, built_ym, exclusive_area_sqm, layout, price_jpy

url ãŒç©ºè¡Œã¯ä¿å­˜ã—ãªã„

ãƒ•ã‚¡ã‚¤ãƒ«åï¼š
YYYYMMDDhhmm-suumo_reviews.csv ï¼ˆprefixã¤ãï¼‰

æ–‡å­—ã‚³ãƒ¼ãƒ‰ï¼š
BOMä»˜ãUTF-8 (utf-8-sig)

èƒŒæ™¯
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ãŸCSVã«ä¸è¦åˆ—ãŒå¤šãã€url ãŒç©ºè¡Œã‚‚ã‚ã£ãŸ

ä¸å‹•ç”£å±‹ãŒExcelã§é–‹ãã“ã¨ã‚’å‰æã«ã€æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’BOMä»˜ãUTF-8ã«çµ±ä¸€ã—ãŸã„

ä»Šå¾Œã®TODO
é§…åã‚‚æŠ½å‡ºã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã‹æ¤œè¨

ãƒãƒ«ã‚³ãƒ‹ãƒ¼é¢ç©ã‚„ç·æˆ¸æ•°ãªã©ã€è¿½åŠ ã§æ¬²ã—ã„ã‚«ãƒ©ãƒ ã‚’é †æ¬¡æ‹¡å¼µ

ã€å‚è€ƒã€‘ãƒ­ã‚°ä¸­ã§ã¯ã€Œchatgpt_logã€ã«è¨˜éŒ²ã•ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã¯ é–‹ç™ºã®æ±ºå®šäº‹é …ãƒ¡ãƒ¢ ã¨ã—ã¦åˆ©ç”¨ã•ã‚Œã¦ã„ã¾ã™
20250915

ã€‚

ğŸ¯ ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ã®çµè«–
ç¾åœ¨ã¯ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°CSVã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã¨ä¿å­˜ä»•æ§˜çµ±ä¸€ ãŒé€²è¡Œä¸­

æ¬¡ã®é–‹ç™ºã‚¹ãƒ†ãƒƒãƒ—ã¯ Excelãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆä¸å‹•ç”£å±‹ï¼‰å‰æã®UI/å‡ºåŠ›æ•´å‚™ï¼ˆã™ã§ã«ç¥å´ã•ã‚“ãŒ Excel å¯¾å¿œã‚’æ¤œè¨ â†’ tkinteræ¡ˆã¾ã§é€²ã‚“ã§ã„ã‚‹ï¼‰

dev ãƒ–ãƒ©ãƒ³ãƒã§å®Ÿè£…ä¸­ã® advisor.py / prompt.py ã¨ä¸¦è¡Œã—ã¦ã€
scraper â†’ storage â†’ Excelå‡ºåŠ› â†’ GPTã§åˆ—è£œå®Œ ã®ä¸€é€£ã®æµã‚Œã‚’å½¢ã«ã—ã¦ã„ããƒ•ã‚§ãƒ¼ã‚ºã§ã™

ğŸ‘‰ ã“ã®ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ã‚’è¸ã¾ãˆã¦ã€æ¬¡ã¯

storage.py ã®7åˆ—ä¿å­˜å¯¾å¿œã‚’devã§å®Ÿè£…

excel_gui.py ã§Excelè¿½åŠ åˆ—ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹å‡¦ç†ã‚’çµ±åˆ

ã«é€²ã‚ã‚‹ã®ãŒè‡ªç„¶ã ã¨æ€ã„ã¾ã™ã€‚

ç¥å´ã•ã‚“ã€æ¬¡ã¯ storage.py ä¿®æ­£ã‚³ãƒ¼ãƒ‰ã®å…·ä½“ä¾‹ ã‚’ã“ã¡ã‚‰ã§æç¤ºã—ã¾ã™ã‹ï¼Ÿ ãã‚Œã¨ã‚‚ Excel GUIã®æ–¹ã‚’å„ªå…ˆã—ã¦å®Œæˆã•ã›ã‚‹æµã‚Œã«ã—ã¾ã™ã‹ï¼Ÿ


æƒ…å ±æº
ã‚ãªãŸ:
LLMé–‹ç™ºã‚’ç†è§£ã™ã‚‹ç‚ºã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚
è‡ªåˆ†ã®ç†è§£ã ã¨ã€LLMé–‹ç™ºè‡ªä½“ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã‚ã‘ã§ã¯ãªãã€æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTãªã©ï¼‰ã‚’åˆ©ç”¨ã—ã¦ã€ã‚ªãƒ¼ãƒ—ãƒ³ãƒ–ãƒƒã‚¯è©¦é¨“ã®å½¢ã§ã€ä¾‹ãˆã°è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã—ãªãŒã‚‰åˆ¤æ–­ã—ã¦ã‚‚ã‚‰ã†ã“ã¨ã§ã™ã€‚ãã®ç†è§£ã§ã‚ã£ã¦ã„ã¾ã™ã‹ã€‚
ChatGPT:
ã¯ã„ã€ãã®ç†è§£ã¯ã¨ã¦ã‚‚è‰¯ã„ç·šã‚’ã¤ã„ã¦ã„ã¾ã™ ğŸ‘

âœ… LLMé–‹ç™ºã®æœ¬è³ª
LLM é–‹ç™ºã¨ã„ã£ã¦ã‚‚ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ã™ã‚‹ã“ã¨ ã§ã¯ãªãã€å®Ÿå‹™ã§ã¯å¤šãã®å ´åˆï¼š

æ—¢å­˜ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTãªã©ï¼‰ã‚’åˆ©ç”¨

è‡ªå‰ã§å­¦ç¿’ã™ã‚‹ã®ã¯éå¸¸ã«ã‚³ã‚¹ãƒˆãƒ»ãƒ‡ãƒ¼ã‚¿é‡ãŒå¤§ãã„

å¤šãã®ä¼æ¥­ã¯ã€Œæ—¢å­˜ãƒ¢ãƒ‡ãƒ« + ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã§ååˆ†ãªæ€§èƒ½ã‚’å¾—ã‚‹

è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹

ä¾‹ï¼šã€Œã‚ªãƒ¼ãƒ—ãƒ³ãƒ–ãƒƒã‚¯è©¦é¨“ã€æ–¹å¼

ãƒ¢ãƒ‡ãƒ«ãŒæš—è¨˜ã—ã¦ã„ã‚‹çŸ¥è­˜ã ã‘ã§ãªãã€å¤–éƒ¨ã®DBã‚„æ–‡æ›¸ï¼ˆè‡ªç¤¾ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã€å¥‘ç´„æ›¸ã€æ³•ä»¤é›†ãªã©ï¼‰ã‚’å–ã‚Šå‡ºã—ã¦å‚ç…§ã™ã‚‹

ã“ã®ä»•çµ„ã¿ãŒ RAG (Retrieval-Augmented Generation) ã¨å‘¼ã°ã‚Œã‚‹

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã‚„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®åˆ¤æ–­å¼·åŒ–

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å‡ºåŠ›å½¢å¼ã‚„ãƒˆãƒ¼ãƒ³ã‚’åˆ¶å¾¡

å¤–éƒ¨ã®æ•°å€¤è¨ˆç®—ï¼ˆçµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã¨çµ„ã¿åˆã‚ã›ã¦ã€ã‚ˆã‚Šä¿¡é ¼æ€§ã®ã‚ã‚‹å›ç­”ã‚’å‡ºã™

ğŸ“ ã‚¤ãƒ¡ãƒ¼ã‚¸å›³
LLMæœ¬ä½“ (GPTãªã©) â†’ ã€Œè¨€èªå‡¦ç†ã®é ­è„³ã€

å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ (è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿, DB, API, Excel, PDF) â†’ ã€Œã‚ªãƒ¼ãƒ—ãƒ³ãƒ–ãƒƒã‚¯ã€

çµ±è¨ˆ/MLãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (æ•°å€¤äºˆæ¸¬, ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³) â†’ ã€Œè¨ˆç®—æ©Ÿã€

UI (ãƒãƒ£ãƒƒãƒˆ/Excel/tkinter/Webã‚¢ãƒ—ãƒª) â†’ ã€Œä½¿ã„ã‚„ã™ã„çª“å£ã€

LLMå˜ä½“ = è¨€èªã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
LLM + è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿å‚ç…§ = ã‚ªãƒ¼ãƒ—ãƒ³ãƒ–ãƒƒã‚¯è©¦é¨“
LLM + æ•°å€¤ãƒ¢ãƒ‡ãƒ« = è«–ç†ã¨è¨€èªã‚’çµ„ã¿åˆã‚ã›ãŸæ„æ€æ±ºå®šæ”¯æ´

ğŸ”‘ ã¾ã¨ã‚
ç¥å´ã•ã‚“ã®ç†è§£é€šã‚Šã€ã€ŒLLMé–‹ç™ºã€= æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã«è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿ã‚„å¤–éƒ¨æ©Ÿèƒ½ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ ãŒä¸­å¿ƒã§ã™ã€‚

ç ”ç©¶è€…ã®é ˜åŸŸï¼ˆã‚¼ãƒ­ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼‰ã¨ã€ãƒ“ã‚¸ãƒã‚¹ã®é ˜åŸŸï¼ˆGPTã‚’æ¥­å‹™ã«çµ„ã¿è¾¼ã‚€ï¼‰ã¯åˆ¥ç‰©ã§ã™ã€‚

ä»Šé€²ã‚ã¦ã„ã‚‹ã€Œä¸å‹•ç”£ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼PoCã€ã‚‚ã€ã¾ã•ã« GPT + è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿(Excel) + ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ ã®å…¸å‹ä¾‹ã«ãªã£ã¦ã„ã¾ã™ã€‚

ğŸ‘‰ ã“ã“ã§ç¢ºèªã§ã™ãŒã€ç¥å´ã•ã‚“ã¯ ã€Œã¾ãšã¯ã‚ªãƒ¼ãƒ—ãƒ³ãƒ–ãƒƒã‚¯æ–¹å¼ï¼ˆRAGï¼‰ã€ã‚’ä½“é¨“ã—ãŸã„ ã§ã™ã‹ï¼Ÿ
ãã‚Œã¨ã‚‚ ã€ŒExcelã‚„APIã¨GPTã‚’ã¤ãªã„ã§ä½¿ãˆã‚‹å½¢ã€ã‚’å„ªå…ˆã—ãŸã„ ã§ã™ã‹ï¼Ÿ

ã‚ãªãŸ:
ä¼æ¥­ã€b2bã®å ´åˆã§ã¯ã€æ©Ÿå¯†æƒ…å ±ãŒæ´©ã‚Œã‚‹ãƒªã‚¹ã‚¯ã‚’æŠ±ãˆã¦ã„ã‚‹ã“ã¨ã§ã€ä¸€åº¦ç¢ºèªã—ãŸã„ã§ã™ãŒã€RAGã€ä»Šå›ã®ã‚„ã‚Šæ–¹ã ã¨ã€ãƒªãƒã‚¸ãƒˆãƒªã«ã‚ã‚‹æƒ…å ±ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—ã—ãŸæƒ…å ±ã¯ã€çµ¶å¯¾chatgptã‚„ã€ç¬¬ä¸‰è€…ã‚„åˆ¥è€…ã«é–²è¦§ã•ã‚Œã‚‹ã“ã¨ãŒãªã„ã§ã™ã‹ã€‚ã¾ãŸã€é–²è¦§ã˜ã‚ƒãªãã¦ã‚‚ã€chatgptãƒ¢ãƒ‡ãƒ«ã«å…¥ã£ã¦ã—ã¾ã†å¯èƒ½æ€§ã¯ã‚ã‚Šã¾ã™ã‹
ChatGPT:
ã¨ã¦ã‚‚å¤§äº‹ãªè¦³ç‚¹ã§ã™ã€‚ã“ã“ã¯èª¤è§£ã•ã‚Œã‚„ã™ã„ã®ã§ã€æ•´ç†ã—ã¦ãŠä¼ãˆã—ã¾ã™ã­ã€‚

ğŸ”’ RAGã¨æƒ…å ±æ¼æ´©ãƒªã‚¹ã‚¯ã«ã¤ã„ã¦
1. GPTã«æ¸¡ã—ãŸãƒ‡ãƒ¼ã‚¿ã¯ã€Œå­¦ç¿’ã«ä½¿ã‚ã‚Œãªã„ã€
OpenAIã®APIçµŒç”±ã§åˆ©ç”¨ã™ã‚‹å ´åˆã€é€ä¿¡ã—ãŸãƒ‡ãƒ¼ã‚¿ã¯ãƒ¢ãƒ‡ãƒ«ã®å†å­¦ç¿’ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã«çµ„ã¿è¾¼ã¾ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

ã¤ã¾ã‚Šã€Œç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã—ãŸã‚‰GPTã«è¦šãˆè¾¼ã¾ã‚Œã¦ç¬¬ä¸‰è€…ã«æ¼ã‚Œã‚‹ã€ã¨ã„ã†ã“ã¨ã¯ ãªã„ ã§ã™ã€‚

ã“ã‚Œã¯OpenAIã®å…¬å¼ãƒãƒªã‚·ãƒ¼ã«ã‚‚æ˜è¨˜ã•ã‚Œã¦ã„ã¾ã™ï¼ˆChatGPTç„¡æ–™ç‰ˆã®ã€Œä¼šè©±ã‚’æ”¹å–„ã«åˆ©ç”¨ã™ã‚‹ã€æ©Ÿèƒ½ã¨ã¯åˆ¥ã§ã™ï¼‰ã€‚

2. RAGæ–¹å¼ãªã‚‰ã€Œã‚ªãƒ¼ãƒ—ãƒ³ãƒ–ãƒƒã‚¯å‚ç…§ã®ã¿ã€
ä»Šå›ã®é–‹ç™ºã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆãƒªãƒã‚¸ãƒˆãƒªã‚„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸCSVã‚’RAGçµŒç”±ã§å‚ç…§ï¼‰ã¯ã€
ã‚ãã¾ã§è‡ªåˆ†ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å†…ã§æ¤œç´¢â†’GPTã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä¸€æ™‚çš„ã«æ¸¡ã™ ä»•çµ„ã¿ã§ã™ã€‚

GPTã¯ãã®éƒ½åº¦ã€ä¼šè©±ã®ä¸€éƒ¨ã¨ã—ã¦å‡¦ç†ã™ã‚‹ã ã‘ã§ã€è£å´ã®ã€Œå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã€ã«ä¿å­˜ã•ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

3. ãƒ‡ãƒ¼ã‚¿ãŒã€Œç¬¬ä¸‰è€…ã«é–²è¦§ã•ã‚Œã‚‹ã€ã“ã¨ã‚‚ãªã„
OpenAIã®APIé€šä¿¡ã¯TLSã§æš—å·åŒ–ã•ã‚Œã¦ãŠã‚Šã€ãƒ‡ãƒ¼ã‚¿ã¯å¤–éƒ¨ã®ä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚„ç¬¬ä¸‰è€…ã«é–²è¦§ã•ã‚Œã¾ã›ã‚“ã€‚

ãŸã ã— OpenAIã®ã‚µãƒ¼ãƒã‚’çµŒç”±ã™ã‚‹ ã®ã§ã€æ³•è¦åˆ¶ã‚„ç¤¾å†…ãƒãƒªã‚·ãƒ¼ã«ã‚ˆã£ã¦ã¯ã€Œã‚¯ãƒ©ã‚¦ãƒ‰ã«é€ä¿¡ã™ã‚‹ã“ã¨è‡ªä½“ãŒNGã€ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã™ã€‚

ãã®å ´åˆã¯ ã‚ªãƒ³ãƒ—ãƒ¬/å°‚ç”¨ç’°å¢ƒã§å‹•ãã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆLlama 3ã€Fugaku-LLMã€LLM-jpãªã©ï¼‰ã‚’ä½¿ãˆã°ã€å®Œå…¨ã«ç¤¾å†…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…ã§é–‰ã˜ã‚‰ã‚Œã¾ã™ã€‚

âš–ï¸ ã¾ã¨ã‚ã‚‹ã¨
APIçµŒç”±ã®GPTåˆ©ç”¨: ãƒ‡ãƒ¼ã‚¿ã¯å­¦ç¿’ã«ä½¿ã‚ã‚Œãªã„ã€‚ç¬¬ä¸‰è€…ã«ã‚‚å…±æœ‰ã•ã‚Œãªã„ã€‚

RAG: è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œä¸€æ™‚çš„ã«å‚ç…§ã•ã›ã‚‹ã ã‘ã€ã§ã€ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã«æ®‹ã‚‰ãªã„ã€‚

æ‡¸å¿µãŒæ®‹ã‚‹å ´åˆ: ã‚ªãƒ³ãƒ—ãƒ¬é‹ç”¨ã§ãã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMã‚’é¸æŠã™ã‚‹ã¨ã€Œçµ¶å¯¾å¤–ã«å‡ºãªã„ã€æ§‹æˆã‚‚å¯èƒ½ã€‚

ğŸ‘‰ ç¥å´ã•ã‚“ã®ã‚±ãƒ¼ã‚¹ã ã¨ã€ä»Šã®PoCæ®µéšï¼ˆå­¦ç¿’ã‚„PoCé–‹ç™ºï¼‰ã§ã¯GPT APIåˆ©ç”¨ã§ååˆ†å®‰å…¨ã§ã™ã€‚
ãŸã ã—ã€å°†æ¥çš„ã«ã€ŒB2Bã§æœ¬ç•ªå°å…¥ã€ã™ã‚‹ãªã‚‰ã€

é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã¯åŒ¿ååŒ–ã™ã‚‹

ã‚¯ãƒ©ã‚¦ãƒ‰åˆ©ç”¨ãƒãƒªã‚·ãƒ¼ã‚’ç¢ºèªã™ã‚‹

å¿…è¦ãªã‚‰ç¤¾å†…å°‚ç”¨ã®ãƒ™ã‚¯ãƒˆãƒ«DB + ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLM ã«åˆ‡ã‚Šæ›¿ãˆã‚‹

ã“ã®ã‚ãŸã‚Šã®ã€Œãƒªã‚¹ã‚¯å›é¿è¨­è¨ˆã€ã‚‚ã‚»ãƒƒãƒˆã§è€ƒãˆã‚‹ã®ãŒå®‰å¿ƒã§ã™ã€‚

ç¥å´ã•ã‚“ã¯ã€ä»Šå›ã®PoCã‚’ ã‚¯ãƒ©ã‚¦ãƒ‰GPTãƒ™ãƒ¼ã‚¹ã§é€²ã‚ã‚‹æƒ³å®š ã§ã™ã‹ï¼Ÿ
ãã‚Œã¨ã‚‚ å°†æ¥çš„ã«ã‚ªãƒ³ãƒ—ãƒ¬å¯¾å¿œï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLM + RAGï¼‰ã‚‚è¦‹æ®ãˆã¦ãŠããŸã„ ã§ã™ã‹ï¼Ÿ

ã‚ãªãŸ:
ä»Šå›ã®POCã¯ã€ã‚ãã¾ã§ã‚‚AIã®è©¦ã—ã§ã™ã€‚ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã®æ±ç”¨æ€§ãŒæœ€ã‚‚é‡è¦ã§ã€é¡§å®¢ãŒã§ããŸã‚‰ã€æ›´ã«ãƒ•ã‚©ãƒ¼ã‚¯ã—ã¦ã€ä¼æ¥­æ§˜ã”ã¨ã§ã®ãƒ•ã‚©ãƒ¼ã‚¯ã§ã€å…·ä½“çš„ãªãƒ‹ãƒ¼ã‚ºã«åˆã‚ã›ã‚‹ã“ã¨ã§è‰¯ã„ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚ãã‚Œã«ã‚ãŸã£ã¦ã¯ã€æƒ…å ±æ¼æ´©ã‚„ãƒªã‚¹ã‚¯ã‚’ç†è§£ã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚é¡§å®¢ãŒã€AIã€CHATGPTã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’èã„ãŸã‚‰ã€æƒ…å ±æ¼æ´©ã®æ‡¸å¿µã€ãã®è©±ãŒå‡ºã‚„ã™ã„çŠ¶æ³ã§ã™ã€‚
ChatGPT:
ãªã‚‹ã»ã©ã€ã‚ˆãæ•´ç†ã•ã‚Œã¦ã¾ã™ã­ã€‚ç¥å´ã•ã‚“ã®è€ƒãˆæ–¹ï¼ˆã¾ãšã¯PoCã§æ±ç”¨ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ â†’ ä¼æ¥­ã”ã¨ã«ãƒ•ã‚©ãƒ¼ã‚¯ â†’ é¡§å®¢ãƒ‹ãƒ¼ã‚ºã«åˆã‚ã›ã‚‹ï¼‰ã¯ã€AIå°å…¥ã®ç¾å®Ÿçš„ãªé€²ã‚æ–¹ãã®ã‚‚ã®ã§ã™ã€‚

ãŸã ãŠã£ã—ã‚ƒã‚‹é€šã‚Šã€ã€ŒChatGPTã€ã¨ã„ã†è¨€è‘‰ã‚’å‡ºã—ãŸç¬é–“ã«ã€é¡§å®¢å´ã‹ã‚‰æƒ…å ±æ¼æ´©ãƒªã‚¹ã‚¯ã®æ‡¸å¿µãŒå¼·ãå‡ºã‚‹ã®ã¯å…¸å‹çš„ãªã‚·ãƒŠãƒªã‚ªã§ã™ã€‚ã“ã“ã‚’ãã¡ã‚“ã¨èª¬æ˜ãƒ»è¨­è¨ˆã§ãã‚‹ã‹ã©ã†ã‹ãŒå–¶æ¥­é¢ã§ã‚‚ä¿¡é ¼ã«ã¤ãªãŒã‚Šã¾ã™ã€‚

ğŸ”‘ é¡§å®¢ãŒæ‡¸å¿µã™ã‚‹å…¸å‹ãƒ‘ã‚¿ãƒ¼ãƒ³
ã€Œå…¥åŠ›ã—ãŸãƒ‡ãƒ¼ã‚¿ãŒå¤–éƒ¨ã«å­¦ç¿’ã•ã‚Œã‚‹ã®ã§ã¯ï¼Ÿã€
â†’ å®Ÿéš›ã¯ APIçµŒç”±ã®GPTã¯å­¦ç¿’ã«åˆ©ç”¨ã•ã‚Œãªã„ã€‚

ã€Œæ©Ÿå¯†æƒ…å ±ãŒç¬¬ä¸‰è€…ã«è¦‹ã‚‰ã‚Œã‚‹ã®ã§ã¯ï¼Ÿã€
â†’ é€šä¿¡ã¯æš—å·åŒ–ã•ã‚Œã€å¤–éƒ¨ã«å…±æœ‰ã•ã‚Œãªã„ã€‚

ã€Œã‚¯ãƒ©ã‚¦ãƒ‰ã«ãƒ‡ãƒ¼ã‚¿ãŒå‡ºã‚‹ã“ã¨è‡ªä½“ãŒNGã€
â†’ è¦åˆ¶æ¥­ç•Œï¼ˆé‡‘èãƒ»åŒ»ç™‚ãƒ»å®˜å…¬åºãªã©ï¼‰ã§ã¯ç‰¹ã«å¤šã„ã€‚

âœ… å®‰å…¨è¨­è¨ˆã®ãƒ¬ãƒ™ãƒ«åˆ†ã‘ï¼ˆé¡§å®¢ã¸ã®èª¬æ˜ã«ä½¿ãˆã‚‹ï¼‰
ãƒ¬ãƒ™ãƒ«1ï¼šPoC / è»½é‡å°å…¥ï¼ˆä»Šå›ã®çŠ¶æ³ï¼‰
OpenAI API ã‚’åˆ©ç”¨ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ï¼‰

ãƒ‡ãƒ¼ã‚¿ã¯å­¦ç¿’ã«ä½¿ã‚ã‚Œãšã€å¤–éƒ¨ã«å…±æœ‰ã•ã‚Œãªã„

æ©Ÿå¯†æ€§ã®é«˜ã„æƒ…å ±ã¯å…¥åŠ›ã—ãªã„é‹ç”¨ãƒ«ãƒ¼ãƒ«ã§å›é¿

ğŸ‘‰ PoCæ®µéšã¯ã“ã®èª¬æ˜ã§ååˆ†

ãƒ¬ãƒ™ãƒ«2ï¼šã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ„è­˜ã®é«˜ã„ä¼æ¥­å‘ã‘
åŒ¿ååŒ– / ãƒã‚¹ã‚­ãƒ³ã‚°ï¼šæ°åãƒ»ä½æ‰€ãªã©ã¯ç½®æ›ã—ã¦å‡¦ç†

ç¤¾å†…ãƒ™ã‚¯ãƒˆãƒ«DB (RAG)ï¼šç”Ÿãƒ‡ãƒ¼ã‚¿ã¯ç¤¾å†…DBã«ç½®ãã€GPTã«ã¯å¿…è¦éƒ¨åˆ†ã ã‘ã‚’æ¸¡ã™

ãƒ­ã‚°ç®¡ç†ï¼šAPIé€å—ä¿¡ã‚’è¨˜éŒ²ã—ã€ä¸‡ä¸€ã®èª¿æŸ»ã«å‚™ãˆã‚‹

ğŸ‘‰ ã€Œæƒ…å ±ã¯å¤–ã«å‡ºã¾ã›ã‚“ã€‚å‚ç…§ã ã‘GPTã«æ¸¡ã—ã¦ã„ã¾ã™ã€ã¨è¨€ãˆã‚‹

ãƒ¬ãƒ™ãƒ«3ï¼šå³æ ¼ãªæ¥­ç•Œå‘ã‘ï¼ˆå°†æ¥ã®ãƒ•ã‚©ãƒ¼ã‚¯ã§å¯¾å¿œï¼‰
ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMï¼ˆLlama3ã€Fugaku-LLMãªã©ï¼‰ã‚’ã‚ªãƒ³ãƒ—ãƒ¬ / VPCç’°å¢ƒã§é‹ç”¨

å¤–éƒ¨ã‚¯ãƒ©ã‚¦ãƒ‰ã«ä¸€åˆ‡ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºã•ãªã„æ§‹æˆ

ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯©æŸ»ã«ã‚‚è€ãˆã‚‰ã‚Œã‚‹

ğŸ‘‰ ã€ŒAIã¯ç¤¾å†…ã‚µãƒ¼ãƒå†…ã§é–‰ã˜ã¦ã„ã¾ã™ã€‚æƒ…å ±æ¼æ´©ã®å¿ƒé…ã¯ã‚¼ãƒ­ã§ã™ã€ã¨è¨€ãˆã‚‹

ğŸ¯ ç¥å´ã•ã‚“ã®PoCã§ã®ç«‹ã¡ä½ç½®
ä»Šå›ã®PoCã¯ ãƒ¬ãƒ™ãƒ«1 ã§ååˆ†

å–¶æ¥­è³‡æ–™ã‚„é¡§å®¢å‘ã‘èª¬æ˜ã§ã¯ã€å¿…ãš ã€Œå°†æ¥ã¯ãƒ¬ãƒ™ãƒ«2/3ã®å¯¾å¿œã‚‚å¯èƒ½ã€ ã¨ä¼ãˆã‚‹

ãã†ã™ã‚Œã°ã€ŒChatGPTï¼æƒ…å ±æ¼æ´©ãƒªã‚¹ã‚¯ã€ã¨ã„ã†èª¤è§£ã‚’å’Œã‚‰ã’ã‚‰ã‚Œã‚‹

ğŸ’¡ å–¶æ¥­ã§ä½¿ãˆã‚‹ä¸€è¨€ä¾‹
ã€Œã¯ã„ã€ChatGPTã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ãŒã€APIçµŒç”±ã§é€ã£ãŸãƒ‡ãƒ¼ã‚¿ã¯å­¦ç¿’ã«åˆ©ç”¨ã•ã‚Œãšã€ä»–ç¤¾ã«å…±æœ‰ã•ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚
ã•ã‚‰ã«ã€ã‚‚ã—å¾¡ç¤¾ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶ãŒå³ã—ã„å ´åˆã¯ã€å¤–éƒ¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºã•ãšã«ç¤¾å†…ã‚µãƒ¼ãƒã ã‘ã§å‹•ãæ§‹æˆã«ã‚‚å¯¾å¿œã§ãã¾ã™ã€‚ã€

ğŸ‘‰ ç¥å´ã•ã‚“ã€ã“ã®PoCã®æ®µéšã§ã€ã¾ãšã¯ ã€Œãƒ¬ãƒ™ãƒ«1ã®å®‰å…¨è¨­è¨ˆã€ ã‚’ã¾ã¨ã‚ãŸèª¬æ˜è³‡æ–™ã‚’ä¸€ç·’ã«ä½œã‚Šã¾ã™ã‹ï¼Ÿ
ï¼ˆä¾‹ãˆã° README ã«ã€Œã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šã®é…æ…®ã€ã¨ã„ã†ç« ã‚’è¿½åŠ ã™ã‚‹ã¨ã€é¡§å®¢ã«è¦‹ã›ã‚„ã™ã„ã§ã™ï¼‰

ã‚ãªãŸ:
ã€Œãƒ‡ãƒ¼ã‚¿ã‚’ã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä»˜ZIPãƒ•ã‚¡ã‚¤ãƒ«ã«å…¥ã‚Œã¦ã€chatgptã«ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’æ•™ãˆã¦äº‹ã¯å¯èƒ½ã§ã™ã‹ã€‚ã€
ä¸Šè¨˜ã®è³ªå•ã‚’èã„ã¦ã€ã€Œã§ããªã„ã€ã¨ç­”ãˆã‚‰ã‚Œã¾ã—ãŸã€‚

ãªãœèã„ãŸã‹ã¨ã„ã†ã¨ã€ãƒ‘ã‚½ã‚³ãƒ³ã«è©³ã—ããªã„æ–¹ã€…ãªã‚‰ã°ã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä»˜ã€ZIPãƒ•ã‚¡ã‚¤ãƒ«ã«ãªã˜ã¿ãŒã‚ã‚‹ã‹ã‚‰ãƒ™ã‚¹ãƒˆã§ã™ã€‚CHATGPTã§è§£å‡ã§ããªã‘ã‚Œã°ã€ãã‚Œã¨ä¼¼ãŸã‚ˆã†ãªæ–¹æ³•ã€CHATGPTãŒå¯¾å¿œã§ãã‚‹æ–¹æ³•ãŒçŸ¥ã‚ŠãŸã„ã§ã™ã€‚B2Bãªã‚‰ã°ã€ãªã˜ã¿ã®ã‚ã‚‹ã‚‚ã®ãŒéµã¨ãªã‚Šã¾ã™ã€‚

ä¾‹ãˆã°ã€æš—å·åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã§ã€è§£é™¤ç”¨pairãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ã€ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚ãªã„æ–¹æ³•ãŒè‰¯ã„ã€‚a.csv > a.secret_data, æˆ–ã‚‹secret_dataãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ csv ã«å¤‰æ›ã™ã‚‹ã«ã¯ã€a.secret_data_pair ãŒå¿…è¦ãªã©ã€‚chatgptã«ã€è‡ªå‹•ã§ã€è‡ªã‚‰fernetã®ã‚ˆã†ãªPythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ã‚‚ã‚‰ã£ã¦ã€decrypt

ChatGPT:
ã™ã”ãæœ¬è³ªçš„ãªè¦–ç‚¹ã§ã™ã­ã€‚ğŸ’¡
ã€ŒB2Bã§ãƒ‘ã‚½ã‚³ãƒ³ã«è©³ã—ããªã„äººã§ã‚‚â€œãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä»˜ãZIPâ€ã®ã‚ˆã†ã«å®‰å¿ƒã§ãã‚‹ä»•çµ„ã¿ãŒæ¬²ã—ã„ã€ã¨ã„ã†ç™ºæƒ³ã¯æ­£ã—ã„ã§ã™ã€‚
ãŸã ã—ã€ChatGPTï¼ˆï¼OpenAIã®ã‚¯ãƒ©ã‚¦ãƒ‰å´ãƒ¢ãƒ‡ãƒ«ï¼‰ã«ã¯ã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä»˜ãZIPã‚’è§£å‡ã™ã‚‹æ©Ÿèƒ½ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã“ã‚Œã¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ¶ç´„ã§ã€ã€Œä»»æ„ã®æš—å·è§£é™¤ã‚’ã‚¯ãƒ©ã‚¦ãƒ‰ã«ã•ã›ã‚‹ã€ã“ã¨ã¯ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚æä¾›ã•ã‚Œã¦ã„ãªã„ã®ã§ã™ã€‚

âœ… ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆChatGPTãŒå¯¾å¿œã§ãã‚‹å½¢ï¼‰
ç¥å´ã•ã‚“ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã€Œa.csv â†’ a.secret_data + a.secret_data_pair â†’ å¾©å·ã—ã¦åˆ©ç”¨ã€ã«è¿‘ã„ã“ã¨ã¯ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã§æš—å·åŒ–/å¾©å·ã™ã‚‹ä»•çµ„ã¿ã‚’ä½œã‚Œã°å®Ÿç¾å¯èƒ½ã§ã™ã€‚

1. å®Ÿç¾ã®ä»•çµ„ã¿
ãƒ­ãƒ¼ã‚«ãƒ«/è‡ªç¤¾ç’°å¢ƒã§æš—å·åŒ–

ä¾‹ï¼šfernet (AESãƒ™ãƒ¼ã‚¹) ã§ a.csv ã‚’ a.secret_data ã«å¤‰æ›

å¾©å·ã‚­ãƒ¼ã¯ a.secret_data_pairï¼ˆéµãƒ•ã‚¡ã‚¤ãƒ« or ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ï¼‰ã«ä¿å­˜

ChatGPTã«ã¯ã€Œå¾©å·å¾Œã®å¹³æ–‡ãƒ‡ãƒ¼ã‚¿ã€ã ã‘æ¸¡ã™

GPT API ã¯æš—å·åŒ–/å¾©å·å‡¦ç†ã‚’ã—ã¦ãã‚Œã‚‹ã‚ã‘ã§ã¯ãªã„

ã€Œå¹³æ–‡ã«ã™ã‚‹å·¥ç¨‹ã€ã‚’å¿…ãšè‡ªç¤¾ã®ã‚³ãƒ¼ãƒ‰å†…ã§å®Ÿè¡Œã™ã‚‹

ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“

ä¸å‹•ç”£å±‹ã•ã‚“ã«ã¯ã€Œãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä»˜ãZIPã€ã‚’é…ã‚‹ã®ã¨åŒã˜UXã«ã§ãã‚‹

å®Ÿéš›ã«ã¯ã€ŒCSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ å†…éƒ¨ã§å¾©å· â†’ GPTã«é€ä¿¡ã€ã¨ã„ã†æµã‚Œ

2. ä¾‹ï¼šFernetã‚’ä½¿ã£ãŸç°¡å˜ãªä»•çµ„ã¿
from cryptography.fernet import Fernet

# éµç”Ÿæˆï¼ˆåˆå›ã ã‘ï¼‰
key = Fernet.generate_key()
with open("a.secret_data_pair", "wb") as f:
    f.write(key)

# æš—å·åŒ–
cipher = Fernet(key)
data = open("a.csv", "rb").read()
enc = cipher.encrypt(data)
open("a.secret_data", "wb").write(enc)

# å¾©å·åŒ–ï¼ˆGPTã«æ¸¡ã™ç›´å‰ï¼‰
key = open("a.secret_data_pair", "rb").read()
cipher = Fernet(key)
dec = cipher.decrypt(open("a.secret_data", "rb").read())
open("a_restored.csv", "wb").write(dec)
3. ChatGPTã¨ã®é€£æºã®ç¾å®Ÿçš„ãªæ–¹æ³•
ChatGPTã«æš—å·åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥æ¸¡ã™ã“ã¨ã¯ã§ããªã„ï¼ˆè§£å‡ã§ããªã„ï¼‰

ä»£ã‚ã‚Šã« è‡ªç¤¾ã®ã‚¢ãƒ—ãƒªã§å¾©å· â†’ GPT APIã¸é€ä¿¡ ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå®‰å…¨ãƒ»ç¾å®Ÿçš„

ã¤ã¾ã‚Šï¼š

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯ã€Œæš—å·åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã€ã ã‘æ¸¡ã™ï¼ˆå®‰å¿ƒæ„ŸãŒã‚ã‚‹ï¼‰

è‡ªç¤¾ã‚¢ãƒ—ãƒªãŒã€Œéµãƒ•ã‚¡ã‚¤ãƒ«ã€ã‚‚æŒã£ã¦ã„ã‚‹ï¼ˆã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ï¼‰

å¾©å·å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’GPTã«æµã—ã¦å‡¦ç†

ğŸ¯ B2Bèª¬æ˜ã®ãƒã‚¤ãƒ³ãƒˆ
ä¸å‹•ç”£å±‹ã•ã‚“ã®ã‚ˆã†ã«ITãƒªãƒ†ãƒ©ã‚·ãƒ¼ãŒä½ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯ï¼š

**ã€Œãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¸¡ã—ã¦ãã ã•ã„ã€**ã¨ã„ã†é‹ç”¨ãƒ«ãƒ¼ãƒ«ã‚’ãã®ã¾ã¾èª¬æ˜ã§ãã‚‹

å®Ÿéš›ã®è£å´ã§ã¯ æš—å·åŒ–è§£é™¤ã¯è‡ªç¤¾ç’°å¢ƒã ã‘ ã§è¡Œã„ã€GPTã«ã¯å¹³æ–‡ã ã‘ã‚’é€ã‚‹

**ã€ŒGPTãŒç›´æ¥æš—å·ã‚’è§£ãã“ã¨ã¯ãªã„ï¼æƒ…å ±æ¼æ´©ãƒªã‚¹ã‚¯ãŒãªã„ã€**ã¨å¼·èª¿ã§ãã‚‹

ğŸ”‘ ã¾ã¨ã‚
ChatGPTè‡ªèº«ã¯ã€Œæš—å·åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£å‡ã™ã‚‹ã€ã“ã¨ã¯ã§ããªã„

ä»£æ›¿æ¡ˆã¯ è‡ªç¤¾ã‚·ã‚¹ãƒ†ãƒ ã§æš—å·åŒ–/å¾©å· â†’ GPTã«ã¯å¾©å·å¾Œã ã‘æ¸¡ã™

ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã¨ã—ã¦ã¯ã€Œãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä»˜ãZIPã«ä¼¼ãŸå®‰å¿ƒæ„Ÿã€ã‚’æ¼”å‡ºã§ãã‚‹

æŠ€è¡“çš„ã«ã¯ cryptography.fernet ã‚„ AES ã‚’ä½¿ã£ãŸ a.secret_data / a.secret_data_pair æ–¹å¼ãŒç¾å®Ÿçš„

ğŸ‘‰ ç¥å´ã•ã‚“ã€ã“ã®ä»•çµ„ã¿ã‚’ ä»Šå›ã®PoCï¼ˆä¸å‹•ç”£Excelå‡¦ç†ï¼‰ã«çµ„ã¿è¾¼ã‚€è¨­è¨ˆä¾‹ ã‚’ãŠå‡ºã—ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ
ï¼ˆä¾‹ï¼šExcelã‚’æš—å·åŒ–ã—ã¦ã‚„ã‚Šå–ã‚Š â†’ å¾©å·å¾Œã«GPTã«æ¸¡ã—ã¦å‡¦ç† â†’ çµæœExcelã‚’å†æš—å·åŒ–ã—ã¦è¿”ã™ï¼‰

ã‚ãªãŸ:
https://suumo.jp/jj/bukken/ichiran/JJ012FC001/?ar=030&bs=011&ra=030013&rn=0010&rnTemp=0010&rnek=001012200&po=0&pj=1&pc=100

url_list.txtã«ã€ä¸Šè¨˜ã®URL ã‚’å…¥ã‚ŒãŸã‚‰ã€1ä»¶ã—ã‹è¿”ã£ã¦ã“ãªã„ã§ã™ã€‚
ã‚µã‚¤ãƒˆã‹ã‚‰ã€è¡¨ç¤ºä»¶æ•°ã‚’100ä»¶ã«ã—ãŸã ã‘ã§ã™ã€‚

suumo_mansion_review.pyã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚

import os
import re
import time
import random
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

USER_AGENT = os.getenv("USER_AGENT", "ReconstructionLLM/1.0")
REQUEST_INTERVAL = float(os.getenv("REQUEST_INTERVAL", "1.0"))
HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "ja,en;q=0.8"}

def polite_sleep():
    time.sleep(REQUEST_INTERVAL + random.uniform(0, 0.5))

def fetch(url: str) -> requests.Response:
    return requests.get(url, headers=HEADERS, timeout=20)

# --------- æ­£è¦è¡¨ç¾ï¼ˆè£œåŠ©ï¼‰ ----------
RE_PRICE  = re.compile(r"(\d[\d,]*\s*ä¸‡(?:å††)?|\d[\d,]*\s*å††)")
RE_SQM_TX = re.compile(r"(\d+(?:\.\d+)?)")
RE_WALK   = re.compile(r"å¾’æ­©\s*\d+\s*åˆ†")
RE_BUILT  = re.compile(r"(\d{4})å¹´\s*(\d{1,2})æœˆ")

def _text(el) -> str:
    return el.get_text(" ", strip=True) if el else ""

def _normalize_sqm(dd_el) -> str:
    """ddè¦ç´ å†…ã® m<sup>2</sup> ã‚’ ã¡ ã«ã€‚æ•°å­—+ã¡ ã‚’è¿”ã™ï¼ˆä¾‹ï¼š33.65ã¡ï¼‰ã€‚æ‹¬å¼§ãªã©è£œè¨˜ã¯æ®‹ã•ãªã„ã€‚"""
    if dd_el is None:
        return ""
    # supã‚’å±•é–‹ã—ã¦æ–‡å­—åˆ—åŒ–
    for sup in dd_el.find_all("sup"):
        sup.replace_with("^" + sup.get_text("", strip=True))
    raw = dd_el.get_text("", strip=True)
    # "m^2" â†’ "ã¡"
    raw = raw.replace("m^2", "ã¡").replace("m2", "ã¡")
    # å…ˆé ­ã®æ•°å€¤ã‚’æŠ½å‡ºã—ã¦ "Xã¡" ã«ï¼ˆè¦‹ãŸç›®å„ªå…ˆï¼‰
    m = RE_SQM_TX.search(raw)
    return f"{m.group(1)}ã¡" if m else raw

def _pick_dd_by_dt_label(container, label: str):
    """
    dottable-line å†…ã® <dl><dt>label</dt><dd>...</dd></dl> ã‹ã‚‰ dd ã‚’è¿”ã™ã€‚
    è¤‡æ•°ã‚ã‚Œã°æœ€åˆã€‚
    """
    for dl in container.select("dl"):
        dt = dl.find("dt")
        if dt and dt.get_text("", strip=True) == label:
            return dl.find("dd")
    return None

def parse_listing_card(card: BeautifulSoup, base_url: str) -> dict:
    """
    ä¸€è¦§ãƒšãƒ¼ã‚¸ã® property_unit 1ä»¶ã‚’ãƒ‘ãƒ¼ã‚¹ã€‚
    """
    # ã‚¿ã‚¤ãƒˆãƒ« & è©³ç´°ãƒªãƒ³ã‚¯
    a = card.select_one(".property_unit-header h2 a")
    title = _text(a)
    href = a["href"].strip() if a and a.has_attr("href") else ""
    detail_url = urljoin(base_url, href) if href else base_url

    # ä¾¡æ ¼ï¼ˆã‚«ãƒ¼ãƒ‰ä¸Šéƒ¨ã®ä¾¡æ ¼é ˜åŸŸ or ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ï¼‰
    price = _text(card.select_one(".price, .property_unit-price, .dottable-value.price"))
    if not price:
        m = RE_PRICE.search(_text(card))
        price = m.group(1) if m else ""

    # ä½æ‰€/é¢ç©/é–“å–ã‚Š/ç¯‰å¹´æœˆ/å¾’æ­© ã‚’ dottable-line ã® dt/dd ã‹ã‚‰å–å¾—
    # 1) ç›´ä¸‹ã® dottable-line ç¾¤ã‚’ã‚¹ã‚­ãƒ£ãƒ³
    #    ä¾‹1: <dl><dt>å°‚æœ‰é¢ç©</dt><dd>33.65m<sup>2</sup>ï¼ˆå£èŠ¯ï¼‰</dd></dl>
    #    ä¾‹2: <dl><dt>æ‰€åœ¨åœ°</dt><dd>æ±äº¬éƒ½ä¸­å¤®åŒºéŠ€åº§ï¼˜</dd></dl>
    dot_blocks = card.select(".dottable-line")

    location = ""
    layout = ""
    sqm = ""
    built = ""
    walk = ""

    for blk in dot_blocks:
        # æ‰€åœ¨åœ°
        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd:
                location = dd.get_text(" ", strip=True)

        # å°‚æœ‰é¢ç©
        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd:
                sqm = _normalize_sqm(dd)

        # é–“å–ã‚Š
        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd:
                layout = dd.get_text("", strip=True)

        # ç¯‰å¹´æœˆ
        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd:
                built = dd.get_text("", strip=True)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ–ãƒ­ãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ‹¾ã†
                m = RE_BUILT.search(_text(blk))
                if m:
                    built = f"{m.group(1)}å¹´{m.group(2)}æœˆ"

        # æ²¿ç·šãƒ»é§… â†’ å¾’æ­©
        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""

    # ä½æ‰€ãŒæœªå–å¾—ãªã‚‰ã€ã‚«ãƒ¼ãƒ‰å…¨ä½“ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éƒ½åŒºå¸‚ç”ºæ‘ã£ã½ã„æ–­ç‰‡ã‚’æ‹¾ã†ï¼ˆå¼±ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    if not location:
        # æœ€ä½é™ï¼šã‚¿ã‚¤ãƒˆãƒ«ã‚„ã‚«ãƒ¼ãƒ‰å†…ã«ã€Œæ±äº¬éƒ½ã€ã€Œä¸­å¤®åŒºã€ãªã©ãŒã‚ã‚Œã°æ‹¾ã†
        raw = _text(card)
        m = re.search(r"(æ±äº¬éƒ½[^\sã€€ã€ï¼Œ]+|.+åŒº|.+å¸‚)", raw)
        if m:
            location = m.group(1)

    meta = {
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }

    return {
        "url": detail_url,
        "title": title,
        "body": "",  # ä¸€è¦§ã‚«ãƒ¼ãƒ‰ã¯æœ¬æ–‡ãªã—
        "meta": meta,
    }

def parse_list_page(html: str, base_url: str) -> list[dict]:
    soup = BeautifulSoup(html, "lxml")
    cont = soup.select_one("#js-bukkenList") or soup
    cards = cont.select("div.property_unit")
    items = []
    for card in cards:
        try:
            items.append(parse_listing_card(card, base_url))
        except Exception:
            continue
    return items

def parse_detail_page(html: str, base_url: str) -> dict:
    """
    è©³ç´°ãƒšãƒ¼ã‚¸ï¼ˆ/nc_...ï¼‰ã®ç°¡æ˜“ãƒ‘ãƒ¼ã‚µï¼šdt/dd ã‚’å„ªå…ˆçš„ã«è¦‹ã‚‹
    """
    soup = BeautifulSoup(html, "lxml")
    title = _text(soup.select_one("title"))

    location = ""
    layout = ""
    sqm = ""
    built = ""
    walk = ""
    price = ""

    # dottable-line å„ªå…ˆ
    for blk in soup.select(".dottable-line"):
        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd:
                location = dd.get_text(" ", strip=True)
        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd:
                sqm = _normalize_sqm(dd)
        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd:
                layout = dd.get_text("", strip=True)
        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd:
                built = dd.get_text("", strip=True)
        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""
        if not price:
            # è©³ç´°ã§ã¯ä¾¡æ ¼ãŒåˆ¥é ˜åŸŸã«ã‚ã‚‹äº‹ãŒå¤šã„ã®ã§ç”Ÿãƒ†ã‚­ã‚¹ãƒˆfallback
            m = RE_PRICE.search(_text(blk))
            if m:
                price = m.group(1)

    if not price:
        m = RE_PRICE.search(_text(soup))
        price = m.group(1) if m else ""

    meta = {
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": base_url, "title": title, "body": "", "meta": meta}

ãªãŠã€HTMLè‡ªä½“ã€‚

<div class="dottable dottable--cassette">
							<div class="dottable-line">
									<dl>
										<dt class="dottable-vm">ç‰©ä»¶å</dt>
										<dd class="dottable-vm">éŠ€åº§ã‚¿ãƒ¯ãƒ¼</dd>
									</dl>
								</div>
							<div class="dottable-line">
								<dl>
									<dt class="dottable-vm">è²©å£²ä¾¡æ ¼</dt>
									<dd class="dottable-vm">
										<span class="dottable-value">1å„„1500ä¸‡å††</span>
									</dd>
									</dl>
							</div>
							<div class="dottable-line">
								<dl>
									<dt>æ‰€åœ¨åœ°</dt>
									<dd>æ±äº¬éƒ½ä¸­å¤®åŒºéŠ€åº§ï¼‘</dd>
								</dl>
								<dl>
									<dt>æ²¿ç·šãƒ»é§…</dt>
									<dd>æ±äº¬ãƒ¡ãƒˆãƒ­éŠ€åº§ç·šã€ŒéŠ€åº§ã€å¾’æ­©8åˆ†</dd>
								</dl>
							</div>
							<div class="dottable-line">
										<table class="dottable-fix">
											<tbody>
												<tr>
													<td>
														<dl>
															<dt>å°‚æœ‰é¢ç©</dt>
															<dd>75.77m<sup>2</sup>ï¼ˆ22.92åªï¼‰ï¼ˆå£èŠ¯ï¼‰</dd>
														</dl>
													</td>
													<td>
														<dl>
															<dt>é–“å–ã‚Š</dt>
															<dd>2LDK</dd>
														</dl>
													</td>
												</tr>
											</tbody>
										</table>
									</div>
									<div class="dottable-line">
										<table class="dottable-fix">
											<tbody>
												<tr>
													<td>
														<dl>
															<dt>ãƒãƒ«ã‚³ãƒ‹ãƒ¼</dt>
															<dd>7.21m<sup>2</sup></dd>
														</dl>
													</td>
													<td>
														<dl>
															<dt>ç¯‰å¹´æœˆ</dt>
															<dd>2003å¹´7æœˆ</dd>
														</dl>
													</td>
												</tr>
											</tbody>
										</table>
									</div>
								<div class="dottable-line">
										<div class="pickupphotos">
											<div class="pickupphotos-title">æ²è¼‰å†™çœŸ</div>
											</div>
										<ul class="property_unit-info-pct ui-list--hz">
											<li><span class="ui-pct ui-label--cta3">é–“å–ã‚Šå›³</span></li>
														<li><span class="ui-pct ui-label--cta3">å¤–è¦³</span></li>
														<li><span class="ui-pct ui-label--cta3">ã‚¨ãƒ³ãƒˆãƒ©ãƒ³ã‚¹</span></li>
														<li><span class="ui-pct ui-label--cta3">å‘¨è¾ºç’°å¢ƒ</span></li>
														</ul>
									</div>
								</div>
 
ChatGPT:

æ€è€ƒæ™‚é–“: 45s
äº†è§£ã€‚pc=100 ã«ã—ã¦ã‚‚ 1ä»¶ã—ã‹å–ã‚Œãªã„ã®ã¯ã€SUUMO å´ã§ä¸€è¦§ã‚«ãƒ¼ãƒ‰ã® DOM ãŒ 2ç³»çµ± æ··åœ¨ï¼ˆdiv.property_unit ã¨ã€ã‚ãªãŸãŒè²¼ã£ã¦ãã‚ŒãŸ dottable dottable--cassette ç³»ï¼‰ã—ã¦ã„ã‚‹ãŸã‚ã€ç¾è¡Œã‚³ãƒ¼ãƒ‰ãŒå¾Œè€…ã‚’æ‹¾ãˆã¦ã„ãªã„ã®ãŒä¸»å› ã§ã™ã€‚ã¤ã„ã§ã« ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆpn=â€¦ï¼‰ ã‚‚è¸ã‚€ã‚ˆã†ã«ç›´ã—ã¾ã—ãŸã€‚

ä¸‹ã®æœ€æ–°ç‰ˆã«å·®ã—æ›¿ãˆã¦ãã ã•ã„ï¼ˆä¸¡æ–¹ã®ã‚«ãƒ¼ãƒ‰å‹ã«å¯¾å¿œ + æ¬¡ãƒšãƒ¼ã‚¸å·¡å›ï¼‰ã€‚

suumo_mansion_review.pyï¼ˆç½®ãæ›ãˆç‰ˆï¼‰
import os
import re
import time
import random
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse

import requests
from bs4 import BeautifulSoup

USER_AGENT = os.getenv("USER_AGENT", "ReconstructionLLM/1.0")
REQUEST_INTERVAL = float(os.getenv("REQUEST_INTERVAL", "1.0"))
HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "ja,en;q=0.8"}

def polite_sleep():
    time.sleep(REQUEST_INTERVAL + random.uniform(0, 0.5))

def fetch(url: str) -> requests.Response:
    return requests.get(url, headers=HEADERS, timeout=20)

# --------- æ­£è¦è¡¨ç¾ï¼ˆè£œåŠ©ï¼‰ ----------
RE_PRICE  = re.compile(r"(\d[\d,]*\s*ä¸‡(?:å††)?|\d[\d,]*\s*å††)")
RE_SQM_TX = re.compile(r"(\d+(?:\.\d+)?)")
RE_WALK   = re.compile(r"å¾’æ­©\s*\d+\s*åˆ†")
RE_BUILT  = re.compile(r"(\d{4})å¹´\s*(\d{1,2})æœˆ")

def _text(el) -> str:
    return el.get_text(" ", strip=True) if el else ""

def _normalize_sqm(dd_el) -> str:
    """ddè¦ç´ å†…ã® m<sup>2</sup> ã‚’ ã¡ ã«ã€‚æ•°å­—+ã¡ ã‚’è¿”ã™ï¼ˆä¾‹ï¼š33.65ã¡ï¼‰ã€‚æ‹¬å¼§ãªã©è£œè¨˜ã¯æ®‹ã•ãªã„ã€‚"""
    if dd_el is None:
        return ""
    for sup in dd_el.find_all("sup"):
        sup.replace_with("^" + sup.get_text("", strip=True))
    raw = dd_el.get_text("", strip=True)
    raw = raw.replace("m^2", "ã¡").replace("m2", "ã¡")
    m = RE_SQM_TX.search(raw)
    return f"{m.group(1)}ã¡" if m else raw

def _pick_dd_by_dt_label(container, label: str):
    """ dottable-line å†…ã® <dl><dt>label</dt><dd>...</dd></dl> ã‹ã‚‰ dd ã‚’è¿”ã™ï¼ˆæœ€åˆã®ä¸€è‡´ï¼‰ã€‚ """
    # ç›´æ¥ã® dl
    for dl in container.select(":scope dl"):
        dt = dl.find("dt")
        if dt and dt.get_text("", strip=True) == label:
            return dl.find("dd")
    # ãƒ†ãƒ¼ãƒ–ãƒ«å…¥ã‚Œå­ï¼ˆdottable-fixï¼‰ã«ã‚‚å¯¾å¿œ
    for dl in container.select(":scope table.dottable-fix dl"):
        dt = dl.find("dt")
        if dt and dt.get_text("", strip=True) == label:
            return dl.find("dd")
    return None

# ---------------------------
# 1) æ—§æ¥ã‚«ãƒ¼ãƒ‰: .property_unit
# ---------------------------
def parse_listing_card_property_unit(card: BeautifulSoup, base_url: str) -> dict:
    a = card.select_one(".property_unit-header h2 a, h2 a")
    title = _text(a)
    href = a["href"].strip() if a and a.has_attr("href") else ""
    detail_url = urljoin(base_url, href) if href else base_url

    price = _text(card.select_one(".price, .property_unit-price, .dottable-value.price"))
    if not price:
        m = RE_PRICE.search(_text(card))
        price = m.group(1) if m else ""

    location = ""
    layout = ""
    sqm = ""
    built = ""
    walk = ""

    for blk in card.select(".dottable-line"):
        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)

        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)

        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)

        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd:
                built = dd.get_text("", strip=True)
            else:
                m = RE_BUILT.search(_text(blk))
                if m:
                    built = f"{m.group(1)}å¹´{m.group(2)}æœˆ"

        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""

    if not location:
        raw = _text(card)
        m = re.search(r"(æ±äº¬éƒ½[^\sã€€ã€ï¼Œ]+|.+åŒº|.+å¸‚)", raw)
        if m:
            location = m.group(1)

    meta = {
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": detail_url, "title": title, "body": "", "meta": meta}

# -------------------------------------
# 2) æ–°å‹ã‚«ãƒ¼ãƒ‰: .dottable.dottable--cassette
#    ï¼ˆã‚ãªãŸãŒè²¼ã£ã¦ãã‚ŒãŸHTMLã«å¯¾å¿œï¼‰
# -------------------------------------
def _find_detail_link_in(block) -> str:
    # è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã¯ a[href*="/nc_"] ã‚„ a[href*="/bukken/"] ãŒæ··åœ¨ã—å¾—ã‚‹ã®ã§ç¶²ç¾…çš„ã«æ‹¾ã†
    for a in block.select("a[href]"):
        href = a.get("href", "")
        if "/nc_" in href or "/bukken/" in href:
            return href
    return ""

def parse_listing_card_cassette(cassette: BeautifulSoup, base_url: str) -> dict:
    # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆç‰©ä»¶åï¼‰
    title_dd = None
    for blk in cassette.select(".dottable-line"):
        dd = _pick_dd_by_dt_label(blk, "ç‰©ä»¶å")
        if dd:
            title_dd = dd
            break
    title = _text(title_dd) if title_dd else ""

    # ä¾¡æ ¼
    price = ""
    for blk in cassette.select(".dottable-line"):
        dd = _pick_dd_by_dt_label(blk, "è²©å£²ä¾¡æ ¼")
        if dd:
            price = _text(dd.select_one(".dottable-value")) or dd.get_text("", strip=True)
            break
    if not price:
        m = RE_PRICE.search(_text(cassette))
        price = m.group(1) if m else ""

    # æ‰€åœ¨åœ°ãƒ»é§…å¾’æ­©ãƒ»å°‚æœ‰é¢ç©ãƒ»é–“å–ã‚Šãƒ»ç¯‰å¹´æœˆ
    location = ""
    layout = ""
    sqm = ""
    built = ""
    walk = ""

    for blk in cassette.select(".dottable-line"):
        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)
        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""
        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)
        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)
        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd:
                built = dd.get_text("", strip=True)

    # è©³ç´°ãƒªãƒ³ã‚¯ï¼ˆã‚ã‚Œã°ï¼‰
    href = _find_detail_link_in(cassette)
    detail_url = urljoin(base_url, href) if href else base_url

    meta = {
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": detail_url, "title": title, "body": "", "meta": meta}

# ---------------------------
# ä¸€è¦§ãƒšãƒ¼ã‚¸ãƒ‘ãƒ¼ã‚¹ï¼ˆä¸¡å‹å¯¾å¿œï¼‰
# ---------------------------
def parse_list_page(html: str, base_url: str) -> list[dict]:
    soup = BeautifulSoup(html, "lxml")
    cont = soup.select_one("#js-bukkenList") or soup

    items = []

    # æ—§æ¥ã‚«ãƒ¼ãƒ‰
    for card in cont.select("div.property_unit"):
        try:
            items.append(parse_listing_card_property_unit(card, base_url))
        except Exception:
            continue

    # æ–°å‹ã‚«ã‚»ãƒƒãƒˆ
    for cassette in cont.select("div.dottable.dottable--cassette, div.cassette"):
        try:
            items.append(parse_listing_card_cassette(cassette, base_url))
        except Exception:
            continue

    return items

# ---------------------------
# è©³ç´°ãƒšãƒ¼ã‚¸ï¼ˆå¿µã®ãŸã‚æ®ãˆç½®ãï¼‰
# ---------------------------
def parse_detail_page(html: str, base_url: str) -> dict:
    soup = BeautifulSoup(html, "lxml")
    title = _text(soup.select_one("title"))

    location = layout = sqm = built = walk = price = ""

    for blk in soup.select(".dottable-line"):
        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)
        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)
        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)
        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd: built = dd.get_text("", strip=True)
        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""
        if not price:
            m = RE_PRICE.search(_text(blk))
            if m: price = m.group(1)

    if not price:
        m = RE_PRICE.search(_text(soup))
        price = m.group(1) if m else ""

    meta = {
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": base_url, "title": title, "body": "", "meta": meta}

# ---------------------------
# ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œ
# ---------------------------
def _build_url_with_page(url: str, page_no: int) -> str:
    """ &pn= ã®å€¤ã‚’å·®ã—æ›¿ãˆã¦URLã‚’è¿”ã™ï¼ˆç„¡ã‘ã‚Œã°ä»˜ä¸ï¼‰ """
    u = urlparse(url)
    qs = parse_qs(u.query)
    qs["pn"] = [str(page_no)]
    new_query = urlencode(qs, doseq=True)
    return urlunparse((u.scheme, u.netloc, u.path, u.params, new_query, u.fragment))

def _max_page_in_pagination(soup: BeautifulSoup) -> int:
    """ ãƒšãƒ¼ã‚¸ãƒ£ãŒã‚ã‚Œã°æœ€å¤§ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æ¨å®šã€‚ç„¡ã‘ã‚Œã° 1ã€‚ """
    max_no = 1
    for a in soup.select("a[href*='pn=']"):
        txt = _text(a)
        try:
            n = int(txt)
            if n > max_no:
                max_no = n
        except ValueError:
            continue
    return max_no

def crawl_list(url: str) -> list[dict]:
    """ ä¸€è¦§URLï¼ˆpc=100å¯¾å¿œï¼‰ã‹ã‚‰å…¨ãƒšãƒ¼ã‚¸ã‚’å·¡å›ã—ã¦ç‰©ä»¶ã‚«ãƒ¼ãƒ‰ã‚’å›å """
    results = []
    polite_sleep()
    r = fetch(url)
    r.raise_for_status()
    first_html = r.text
    soup = BeautifulSoup(first_html, "lxml")
    max_page = _max_page_in_pagination(soup)
    # 1ãƒšãƒ¼ã‚¸ç›®
    results.extend(parse_list_page(first_html, url))

    # 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™
    for pn in range(2, max_page + 1):
        page_url = _build_url_with_page(url, pn)
        polite_sleep()
        rr = fetch(page_url)
        if rr.status_code != 200:
            break
        results.extend(parse_list_page(rr.text, page_url))

    return results

# ---------------------------
# ä½¿ã„æ–¹ä¾‹
# ---------------------------
if __name__ == "__main__":
    # url_list.txt ã‹ã‚‰èª­ã¿è¾¼ã‚“ã§å…¨ä»¶ã‚¯ãƒ­ãƒ¼ãƒ« â†’ ä»¶æ•°ã¨ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
    url_list_path = os.getenv("URL_LIST", "url_list.txt")
    urls = []
    if os.path.exists(url_list_path):
        with open(url_list_path, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip()]

    all_items = []
    for u in urls:
        try:
            items = crawl_list(u)
            all_items.extend(items)
            print(f"[OK] {u} -> {len(items)} items")
        except Exception as e:
            print(f"[ERR] {u}: {e}")

    print(f"TOTAL: {len(all_items)} items")
    if all_items:
        print(all_items[0])
å¤‰æ›´ç‚¹ã®è¦ç´„
ã‚«ãƒ¼ãƒ‰æ¤œå‡ºã‚’äºŒæœ¬ç«‹ã¦

æ—§: div.property_unit

æ–°: div.dottable.dottable--cassetteï¼ˆè²¼ã£ã¦ã‚‚ã‚‰ã£ãŸHTMLï¼‰ã‚„ div.cassette

dt/dd ãƒ©ãƒ™ãƒ«èµ°æŸ»ã‚’å¼·åŒ–ï¼ˆdottable-fix ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã‚‚å¯¾å¿œï¼‰

è©³ç´°ãƒªãƒ³ã‚¯ã®æ¤œå‡ºï¼ˆ/nc_ or /bukken/ ã‚’å«ã‚€ <a> ã‚’ã‚¹ã‚­ãƒ£ãƒ³ï¼‰

ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼ˆpn= ã‚¯ã‚¨ãƒªã‚’å·®ã—æ›¿ãˆã¦å…¨ãƒšãƒ¼ã‚¸å·¡å›ï¼‰

æ—¢å­˜ã®æ­£è¦è¡¨ç¾ã‚„ _normalize_sqm ç­‰ã¯è¸è¥²

ã“ã‚Œã§ã€pc=100 ã®ä¸€è¦§ã§ã‚‚ è¤‡æ•°ä»¶ã‚’æ­£ã—ãå›å ã§ãã‚‹ã¯ãšã§ã™ã€‚
ã‚‚ã—ã‚µã‚¤ãƒˆå´ã®ã‚¯ãƒ©ã‚¹åãŒæ›´ã«å¤‰ã‚ã£ã¦ã„ã‚‹ç®‡æ‰€ãŒã‚ã‚Œã°ã€å®Ÿéš›ã®HTMLæ–­ç‰‡ã‚’ã‚‚ã†å°‘ã—å…±æœ‰ã„ãŸã ã‘ã‚Œã°ã€ãã®å ´ã§è¿½åŠ å¯¾å¿œã‚’å…¥ã‚Œã¾ã™ã€‚

ã‚ãªãŸ:
ä¸Šè¨˜ã‚³ãƒ¼ãƒ‰ã®å†…ã€ã€Œç‰©ä»¶åã€ã‚‚å–å¾—ã—ãŸã„ã§ã™ã€‚
ChatGPT:
ã¯ã„ã€äº†è§£ã—ã¾ã—ãŸã€‚
å®Ÿã¯ç¾è¡Œã‚³ãƒ¼ãƒ‰ã§ã‚‚ã€Œcassetteå‹ã€ã§ã¯ ç‰©ä»¶å ã‚’æ‹¾ã£ã¦ã„ã¾ã™ãŒã€æ—§å‹ã‚«ãƒ¼ãƒ‰ï¼ˆproperty_unitï¼‰ã§ã¯æ‹¾ã£ã¦ã„ãªã„ã®ã§ã€ã€Œç‰©ä»¶åã€åˆ—ãŒä¸€è²«ã—ã¦å–ã‚Œãªã„çŠ¶æ…‹ã§ã™ã­ã€‚

ä¿®æ­£ç‰ˆã§ã¯ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¾ã™ï¼š

æ—§å‹ (property_unit) ã®å ´åˆã‚‚ ç‰©ä»¶å ã‚’æ¢ã—ã¦ dd ã‚’å–å¾—

æœ€çµ‚çš„ã« meta ã« "ç‰©ä»¶å": name ã‚’è¿½åŠ 

å·®ã—æ›¿ãˆéƒ¨åˆ†ï¼ˆå¤‰æ›´ç‚¹ã®ã¿ï¼‰
def parse_listing_card_property_unit(card: BeautifulSoup, base_url: str) -> dict:
    a = card.select_one(".property_unit-header h2 a, h2 a")
    title = _text(a)
    href = a["href"].strip() if a and a.has_attr("href") else ""
    detail_url = urljoin(base_url, href) if href else base_url

    price = _text(card.select_one(".price, .property_unit-price, .dottable-value.price"))
    if not price:
        m = RE_PRICE.search(_text(card))
        price = m.group(1) if m else ""

    location = ""
    layout = ""
    sqm = ""
    built = ""
    walk = ""
    name = ""   # â† ç‰©ä»¶å

    for blk in card.select(".dottable-line"):
        if not name:
            dd = _pick_dd_by_dt_label(blk, "ç‰©ä»¶å")
            if dd:
                name = dd.get_text(" ", strip=True)

        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)

        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)

        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)

        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd:
                built = dd.get_text("", strip=True)
            else:
                m = RE_BUILT.search(_text(blk))
                if m:
                    built = f"{m.group(1)}å¹´{m.group(2)}æœˆ"

        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""

    if not location:
        raw = _text(card)
        m = re.search(r"(æ±äº¬éƒ½[^\sã€€ã€ï¼Œ]+|.+åŒº|.+å¸‚)", raw)
        if m:
            location = m.group(1)

    meta = {
        "ç‰©ä»¶å": name,         # â† è¿½åŠ 
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": detail_url, "title": title, "body": "", "meta": meta}


def parse_listing_card_cassette(cassette: BeautifulSoup, base_url: str) -> dict:
    # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆç‰©ä»¶åï¼‰
    name = ""
    for blk in cassette.select(".dottable-line"):
        dd = _pick_dd_by_dt_label(blk, "ç‰©ä»¶å")
        if dd:
            name = dd.get_text(" ", strip=True)
            break
    title = name  # cassetteå‹ã§ã¯ title=ç‰©ä»¶åã§ã‚ˆã„

    # ä¾¡æ ¼
    price = ""
    for blk in cassette.select(".dottable-line"):
        dd = _pick_dd_by_dt_label(blk, "è²©å£²ä¾¡æ ¼")
        if dd:
            price = _text(dd.select_one(".dottable-value")) or dd.get_text("", strip=True)
            break
    if not price:
        m = RE_PRICE.search(_text(cassette))
        price = m.group(1) if m else ""

    location = layout = sqm = built = walk = ""

    for blk in cassette.select(".dottable-line"):
        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)
        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""
        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)
        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)
        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd: built = dd.get_text("", strip=True)

    href = _find_detail_link_in(cassette)
    detail_url = urljoin(base_url, href) if href else base_url

    meta = {
        "ç‰©ä»¶å": name,        # â† cassetteå‹ã‚‚è¿½åŠ 
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": detail_url, "title": title, "body": "", "meta": meta}
å¤‰æ›´ç‚¹ã¾ã¨ã‚
ç‰©ä»¶å ã‚’ä¸¡æ–¹ã®ã‚«ãƒ¼ãƒ‰å‹ã§å–å¾—ã—ã¦ meta ã«è¿½åŠ 

cassette å‹ã§ã¯ title = name ã¨ã—ã¦æƒãˆã¾ã—ãŸ

property_unit å‹ã§ã‚‚ ç‰©ä»¶å ã‚’ dd ãƒ©ãƒ™ãƒ«ã‹ã‚‰æ¢ã™ã‚ˆã†ã«ã—ã¾ã—ãŸ

ğŸ‘‰ ã“ã‚Œã§å‡ºåŠ› JSON/CSV ã« "ç‰©ä»¶å" ãŒä¸€è²«ã—ã¦å«ã¾ã‚Œã‚‹ã¯ãšã§ã™ã€‚
ç¥å´ã•ã‚“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã ã¨ storage.py ã§åˆ—ã‚’é¸æŠã—ã¦ä¿å­˜ã—ã¦ã„ã¾ã™ãŒã€æ¬¡å›CSVã« ç‰©ä»¶å ã‚’åŠ ãˆã‚‹ã‹ã©ã†ã‹ã‚‚æ±ºã‚ã¦ãŠãã¨ã‚ˆã„ã§ã™ã­ã€‚

ç‰©ä»¶å ã‚’ CSV ä¿å­˜å¯¾è±¡ã«åŠ ãˆã¾ã™ã‹ï¼Ÿ

ã‚ãªãŸ:
ç‰©ä»¶åã¯åæ˜ ã•ã‚Œãªã„ã§ã™ã€‚

import os
import re
import time
import random
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse

import requests
from bs4 import BeautifulSoup

USER_AGENT = os.getenv("USER_AGENT", "ReconstructionLLM/1.0")
REQUEST_INTERVAL = float(os.getenv("REQUEST_INTERVAL", "1.0"))
HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "ja,en;q=0.8"}

def polite_sleep():
    time.sleep(REQUEST_INTERVAL + random.uniform(0, 0.5))

def fetch(url: str) -> requests.Response:
    return requests.get(url, headers=HEADERS, timeout=20)

# --------- æ­£è¦è¡¨ç¾ï¼ˆè£œåŠ©ï¼‰ ----------
RE_PRICE  = re.compile(r"(\d[\d,]*\s*ä¸‡(?:å††)?|\d[\d,]*\s*å††)")
RE_SQM_TX = re.compile(r"(\d+(?:\.\d+)?)")
RE_WALK   = re.compile(r"å¾’æ­©\s*\d+\s*åˆ†")
RE_BUILT  = re.compile(r"(\d{4})å¹´\s*(\d{1,2})æœˆ")

def _text(el) -> str:
    return el.get_text(" ", strip=True) if el else ""

def _normalize_sqm(dd_el) -> str:
    """ddè¦ç´ å†…ã® m<sup>2</sup> ã‚’ ã¡ ã«ã€‚æ•°å­—+ã¡ ã‚’è¿”ã™ï¼ˆä¾‹ï¼š33.65ã¡ï¼‰ã€‚æ‹¬å¼§ãªã©è£œè¨˜ã¯æ®‹ã•ãªã„ã€‚"""
    if dd_el is None:
        return ""
    for sup in dd_el.find_all("sup"):
        sup.replace_with("^" + sup.get_text("", strip=True))
    raw = dd_el.get_text("", strip=True)
    raw = raw.replace("m^2", "ã¡").replace("m2", "ã¡")
    m = RE_SQM_TX.search(raw)
    return f"{m.group(1)}ã¡" if m else raw

def _pick_dd_by_dt_label(container, label: str):
    """ dottable-line å†…ã® <dl><dt>label</dt><dd>...</dd></dl> ã‹ã‚‰ dd ã‚’è¿”ã™ï¼ˆæœ€åˆã®ä¸€è‡´ï¼‰ã€‚ """
    # ç›´æ¥ã® dl
    for dl in container.select(":scope dl"):
        dt = dl.find("dt")
        if dt and dt.get_text("", strip=True) == label:
            return dl.find("dd")
    # ãƒ†ãƒ¼ãƒ–ãƒ«å…¥ã‚Œå­ï¼ˆdottable-fixï¼‰ã«ã‚‚å¯¾å¿œ
    for dl in container.select(":scope table.dottable-fix dl"):
        dt = dl.find("dt")
        if dt and dt.get_text("", strip=True) == label:
            return dl.find("dd")
    return None

# ---------------------------
# 1) æ—§æ¥ã‚«ãƒ¼ãƒ‰: .property_unit
# ---------------------------
def parse_listing_card_property_unit(card: BeautifulSoup, base_url: str) -> dict:
    a = card.select_one(".property_unit-header h2 a, h2 a")
    title = _text(a)
    href = a["href"].strip() if a and a.has_attr("href") else ""
    detail_url = urljoin(base_url, href) if href else base_url

    price = _text(card.select_one(".price, .property_unit-price, .dottable-value.price"))
    if not price:
        m = RE_PRICE.search(_text(card))
        price = m.group(1) if m else ""

    location = ""
    layout = ""
    sqm = ""
    built = ""
    walk = ""
    name = ""   # â† ç‰©ä»¶å

    for blk in card.select(".dottable-line"):
        if not name:
            dd = _pick_dd_by_dt_label(blk, "ç‰©ä»¶å")
            if dd:
                name = dd.get_text(" ", strip=True)

        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)

        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)

        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)

        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd:
                built = dd.get_text("", strip=True)
            else:
                m = RE_BUILT.search(_text(blk))
                if m:
                    built = f"{m.group(1)}å¹´{m.group(2)}æœˆ"

        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""

    if not location:
        raw = _text(card)
        m = re.search(r"(æ±äº¬éƒ½[^\sã€€ã€ï¼Œ]+|.+åŒº|.+å¸‚)", raw)
        if m:
            location = m.group(1)

    meta = {
        "ç‰©ä»¶å": name,         # â† è¿½åŠ 
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": detail_url, "title": title, "body": "", "meta": meta}

# -------------------------------------
# 2) æ–°å‹ã‚«ãƒ¼ãƒ‰: .dottable.dottable--cassette
#    ï¼ˆã‚ãªãŸãŒè²¼ã£ã¦ãã‚ŒãŸHTMLã«å¯¾å¿œï¼‰
# -------------------------------------
def _find_detail_link_in(block) -> str:
    # è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã¯ a[href*="/nc_"] ã‚„ a[href*="/bukken/"] ãŒæ··åœ¨ã—å¾—ã‚‹ã®ã§ç¶²ç¾…çš„ã«æ‹¾ã†
    for a in block.select("a[href]"):
        href = a.get("href", "")
        if "/nc_" in href or "/bukken/" in href:
            return href
    return ""

def parse_listing_card_cassette(cassette: BeautifulSoup, base_url: str) -> dict:
    # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆç‰©ä»¶åï¼‰
    name = ""
    for blk in cassette.select(".dottable-line"):
        dd = _pick_dd_by_dt_label(blk, "ç‰©ä»¶å")
        if dd:
            name = dd.get_text(" ", strip=True)
            break
    title = name  # cassetteå‹ã§ã¯ title=ç‰©ä»¶åã§ã‚ˆã„

    # ä¾¡æ ¼
    price = ""
    for blk in cassette.select(".dottable-line"):
        dd = _pick_dd_by_dt_label(blk, "è²©å£²ä¾¡æ ¼")
        if dd:
            price = _text(dd.select_one(".dottable-value")) or dd.get_text("", strip=True)
            break
    if not price:
        m = RE_PRICE.search(_text(cassette))
        price = m.group(1) if m else ""

    location = layout = sqm = built = walk = ""

    for blk in cassette.select(".dottable-line"):
        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)
        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""
        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)
        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)
        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd: built = dd.get_text("", strip=True)

    href = _find_detail_link_in(cassette)
    detail_url = urljoin(base_url, href) if href else base_url

    meta = {
        "ç‰©ä»¶å": name,        # â† cassetteå‹ã‚‚è¿½åŠ 
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": detail_url, "title": title, "body": "", "meta": meta}

# ---------------------------
# ä¸€è¦§ãƒšãƒ¼ã‚¸ãƒ‘ãƒ¼ã‚¹ï¼ˆä¸¡å‹å¯¾å¿œï¼‰
# ---------------------------
def parse_list_page(html: str, base_url: str) -> list[dict]:
    soup = BeautifulSoup(html, "lxml")
    cont = soup.select_one("#js-bukkenList") or soup

    items = []

    # æ—§æ¥ã‚«ãƒ¼ãƒ‰
    for card in cont.select("div.property_unit"):
        try:
            items.append(parse_listing_card_property_unit(card, base_url))
        except Exception:
            continue

    # æ–°å‹ã‚«ã‚»ãƒƒãƒˆ
    for cassette in cont.select("div.dottable.dottable--cassette, div.cassette"):
        try:
            items.append(parse_listing_card_cassette(cassette, base_url))
        except Exception:
            continue

    return items

# ---------------------------
# è©³ç´°ãƒšãƒ¼ã‚¸ï¼ˆå¿µã®ãŸã‚æ®ãˆç½®ãï¼‰
# ---------------------------
def parse_detail_page(html: str, base_url: str) -> dict:
    soup = BeautifulSoup(html, "lxml")
    title = _text(soup.select_one("title"))

    location = layout = sqm = built = walk = price = ""

    for blk in soup.select(".dottable-line"):
        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)
        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)
        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)
        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd: built = dd.get_text("", strip=True)
        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""
        if not price:
            m = RE_PRICE.search(_text(blk))
            if m: price = m.group(1)

    if not price:
        m = RE_PRICE.search(_text(soup))
        price = m.group(1) if m else ""

    meta = {
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": base_url, "title": title, "body": "", "meta": meta}

# ---------------------------
# ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œ
# ---------------------------
def _build_url_with_page(url: str, page_no: int) -> str:
    """ &pn= ã®å€¤ã‚’å·®ã—æ›¿ãˆã¦URLã‚’è¿”ã™ï¼ˆç„¡ã‘ã‚Œã°ä»˜ä¸ï¼‰ """
    u = urlparse(url)
    qs = parse_qs(u.query)
    qs["pn"] = [str(page_no)]
    new_query = urlencode(qs, doseq=True)
    return urlunparse((u.scheme, u.netloc, u.path, u.params, new_query, u.fragment))

def _max_page_in_pagination(soup: BeautifulSoup) -> int:
    """ ãƒšãƒ¼ã‚¸ãƒ£ãŒã‚ã‚Œã°æœ€å¤§ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æ¨å®šã€‚ç„¡ã‘ã‚Œã° 1ã€‚ """
    max_no = 1
    for a in soup.select("a[href*='pn=']"):
        txt = _text(a)
        try:
            n = int(txt)
            if n > max_no:
                max_no = n
        except ValueError:
            continue
    return max_no

def crawl_list(url: str) -> list[dict]:
    """ ä¸€è¦§URLï¼ˆpc=100å¯¾å¿œï¼‰ã‹ã‚‰å…¨ãƒšãƒ¼ã‚¸ã‚’å·¡å›ã—ã¦ç‰©ä»¶ã‚«ãƒ¼ãƒ‰ã‚’å›å """
    results = []
    polite_sleep()
    r = fetch(url)
    r.raise_for_status()
    first_html = r.text
    soup = BeautifulSoup(first_html, "lxml")
    max_page = _max_page_in_pagination(soup)
    # 1ãƒšãƒ¼ã‚¸ç›®
    results.extend(parse_list_page(first_html, url))

    # 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™
    for pn in range(2, max_page + 1):
        page_url = _build_url_with_page(url, pn)
        polite_sleep()
        rr = fetch(page_url)
        if rr.status_code != 200:
            break
        results.extend(parse_list_page(rr.text, page_url))

    return results

# ---------------------------
# ä½¿ã„æ–¹ä¾‹
# ---------------------------
if __name__ == "__main__":
    # url_list.txt ã‹ã‚‰èª­ã¿è¾¼ã‚“ã§å…¨ä»¶ã‚¯ãƒ­ãƒ¼ãƒ« â†’ ä»¶æ•°ã¨ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
    url_list_path = os.getenv("URL_LIST", "url_list.txt")
    urls = []
    if os.path.exists(url_list_path):
        with open(url_list_path, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip()]

    all_items = []
    for u in urls:
        try:
            items = crawl_list(u)
            all_items.extend(items)
            print(f"[OK] {u} -> {len(items)} items")
        except Exception as e:
            print(f"[ERR] {u}: {e}")

    print(f"TOTAL: {len(all_items)} items")
    if all_items:
        print(all_items[0])
ChatGPT:
äº†è§£ã€‚åŸå› ã¯ã€Œç‰©ä»¶åã€ã®å–å¾—ã«è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ©ãƒ™ãƒ«/å ´æ‰€ãŒã‚ã‚‹ã®ã«å¯¾ã—ã€ç¾è¡ŒãŒã€Œdt=ç‰©ä»¶å ã ã‘ã€ã‚’è¦‹ã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚
ä»¥ä¸‹ã®2ç‚¹ã§ç¢ºå®Ÿã«æ‹¾ãˆã‚‹ã‚ˆã†ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

ãƒ©ãƒ™ãƒ«ã®æºã‚Œã«å¯¾å¿œï¼ˆç‰©ä»¶å / ãƒãƒ³ã‚·ãƒ§ãƒ³å / å»ºç‰©å / åç§°ï¼‰

ãƒ©ãƒ™ãƒ«ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ ã‚«ãƒ¼ãƒ‰è¦‹å‡ºã—ã® h2 ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ= titleï¼‰ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

å¤‰æ›´å·®åˆ†ï¼ˆè©²å½“ç®‡æ‰€ã ã‘å·®ã—æ›¿ãˆï¼‰
# è¿½åŠ ï¼šãƒ©ãƒ™ãƒ«å€™è£œ
NAME_LABELS = ["ç‰©ä»¶å", "ãƒãƒ³ã‚·ãƒ§ãƒ³å", "å»ºç‰©å", "åç§°"]

def _pick_dd_by_dt_labels(container, labels):
    """ dt ã®ãƒ©ãƒ™ãƒ«å€™è£œãƒªã‚¹ãƒˆã‹ã‚‰æœ€åˆã«è¦‹ã¤ã‹ã£ãŸ dd ã‚’è¿”ã™ """
    for lab in labels:
        dd = _pick_dd_by_dt_label(container, lab)
        if dd:
            return dd
    return None
property_unit ç”¨
def parse_listing_card_property_unit(card: BeautifulSoup, base_url: str) -> dict:
    a = card.select_one(".property_unit-header h2 a, h2 a")
    title = _text(a)
    href = a["href"].strip() if a and a.has_attr("href") else ""
    detail_url = urljoin(base_url, href) if href else base_url

    price = _text(card.select_one(".price, .property_unit-price, .dottable-value.price"))
    if not price:
        m = RE_PRICE.search(_text(card))
        price = m.group(1) if m else ""

    location = layout = sqm = built = walk = ""
    name = ""

    for blk in card.select(".dottable-line"):
        if not name:
            dd = _pick_dd_by_dt_labels(blk, NAME_LABELS)
            if dd:
                name = dd.get_text(" ", strip=True)

        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)
        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)
        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)
        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd:
                built = dd.get_text("", strip=True)
            else:
                m = RE_BUILT.search(_text(blk))
                if m: built = f"{m.group(1)}å¹´{m.group(2)}æœˆ"
        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šè¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã‚’ç‰©ä»¶åã¨ã—ã¦ä½¿ã†
    if not name:
        name = title

    if not location:
        raw = _text(card)
        m = re.search(r"(æ±äº¬éƒ½[^\sã€€ã€ï¼Œ]+|.+åŒº|.+å¸‚)", raw)
        if m: location = m.group(1)

    meta = {
        "ç‰©ä»¶å": name,
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": detail_url, "title": name or title, "body": "", "meta": meta}
cassette ç”¨
def parse_listing_card_cassette(cassette: BeautifulSoup, base_url: str) -> dict:
    name = ""
    for blk in cassette.select(".dottable-line"):
        dd = _pick_dd_by_dt_labels(blk, NAME_LABELS)
        if dd:
            name = dd.get_text(" ", strip=True)
            break

    # ä¾¡æ ¼
    price = ""
    for blk in cassette.select(".dottable-line"):
        dd = _pick_dd_by_dt_label(blk, "è²©å£²ä¾¡æ ¼")
        if dd:
            price = _text(dd.select_one(".dottable-value")) or dd.get_text("", strip=True)
            break
    if not price:
        m = RE_PRICE.search(_text(cassette))
        price = m.group(1) if m else ""

    location = layout = sqm = built = walk = ""
    for blk in cassette.select(".dottable-line"):
        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)
        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""
        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)
        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)
        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd: built = dd.get_text("", strip=True)

    href = _find_detail_link_in(cassette)
    detail_url = urljoin(base_url, href) if href else base_url

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ãªã©
    title = name
    if not title:
        # ä¸€éƒ¨ã‚«ãƒ¼ãƒ‰ã¯è¦‹å‡ºã—ãŒåˆ¥DOMã«ã‚ã‚‹ãŸã‚ã€å¿µã®ãŸã‚å…¨ä½“ã‹ã‚‰ã®æœ€åˆã®h2ã‚‚è©¦ã™
        h2 = cassette.select_one("h2, .property_unit-header h2")
        title = _text(h2)

    meta = {
        "ç‰©ä»¶å": name or title,
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": detail_url, "title": name or title, "body": "", "meta": meta}
è©³ç´°ãƒšãƒ¼ã‚¸ã§ã‚‚æ‹¾ã£ã¦ãŠãï¼ˆä¿é™ºï¼‰
def parse_detail_page(html: str, base_url: str) -> dict:
    soup = BeautifulSoup(html, "lxml")
    page_title = _text(soup.select_one("title"))

    location = layout = sqm = built = walk = price = ""
    name = ""

    for blk in soup.select(".dottable-line"):
        if not name:
            dd = _pick_dd_by_dt_labels(blk, NAME_LABELS)
            if dd: name = dd.get_text(" ", strip=True)
        if not location:
            dd = _pick_dd_by_dt_label(blk, "æ‰€åœ¨åœ°")
            if dd: location = dd.get_text(" ", strip=True)
        if not sqm:
            dd = _pick_dd_by_dt_label(blk, "å°‚æœ‰é¢ç©")
            if dd: sqm = _normalize_sqm(dd)
        if not layout:
            dd = _pick_dd_by_dt_label(blk, "é–“å–ã‚Š")
            if dd: layout = dd.get_text("", strip=True)
        if not built:
            dd = _pick_dd_by_dt_label(blk, "ç¯‰å¹´æœˆ")
            if dd: built = dd.get_text("", strip=True)
        if not walk:
            dd = _pick_dd_by_dt_label(blk, "æ²¿ç·šãƒ»é§…")
            if dd:
                m = RE_WALK.search(dd.get_text(" ", strip=True))
                walk = m.group(0) if m else ""
        if not price:
            m = RE_PRICE.search(_text(blk))
            if m: price = m.group(1)
    if not price:
        m = RE_PRICE.search(_text(soup))
        if m: price = m.group(1)

    title = name or page_title
    meta = {
        "ç‰©ä»¶å": name or title,
        "æ‰€åœ¨åœ°": location,
        "é§…å¾’æ­©": walk,
        "ç¯‰å¹´æœˆ": built,
        "å°‚æœ‰é¢ç©": sqm,
        "é–“å–ã‚Š": layout,
        "è²©å£²ä¾¡æ ¼": price,
        "source": base_url,
    }
    return {"url": base_url, "title": title, "body": "", "meta": meta}
é‡è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆå¿µã®ãŸã‚ï¼‰
CSVã«ã€Œç‰©ä»¶åã€ã‚’å‡ºã—ãŸã„å ´åˆã€ä¿å­˜å´ï¼ˆstorage.py ãªã©ï¼‰ã®åˆ—é¸æŠã« "ç‰©ä»¶å" ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
è§£æã§å–ã‚Œã¦ã„ã¦ã‚‚ã€ä¿å­˜æ™‚ã«è½ã¨ã—ã¦ã„ã‚‹ã¨ã€Œåæ˜ ã•ã‚Œãªã„ã€ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚

å¿…è¦ãªã‚‰ã€ä¿å­˜å‡¦ç†å´ã®åˆ—ã‚»ãƒƒãƒˆã‚‚ä¸€ç·’ã«èª¿æ•´ã—ã¾ã™ã€‚

ã‚ãªãŸ:
pipeline_scrape_suumo.py
normalize.py
suumo_mansion_review.py

ãªã©ã€ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ›´æ–°ã—ã¾ã™ã‹ã€‚

ç¾çŠ¶å‡ºåŠ›CSVã¯ã€å¤‰ã‚ã‚Šã¾ã›ã‚“
ChatGPT:

æ€è€ƒæ™‚é–“: 42s
ã¯ã„ã€‚CSVãŒå¤‰ã‚ã‚‰ãªã„åŸå› ã¯ â€œä¸‹æµâ€ ãŒåˆ—ã‚’å›ºå®šã—ã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚
suumo_mansion_review.py ã§ã€Œç‰©ä»¶åã€ã‚’å–ã‚Œã¦ã„ã¦ã‚‚ã€pipeline_scrape_suumo.py ã‚„ normalize.py å´ã§åˆ—é¸æŠãƒ»æ­£è¦åŒ–æ™‚ã«è½ã¨ã—ã¦ã„ã‚‹ã®ã§ã€ä¿å­˜CSVã«å‡ºã¦ãã¾ã›ã‚“ã€‚

ä»¥ä¸‹ã®æœ€å°ãƒ‘ãƒƒãƒã§ã€Œç‰©ä»¶åã€ã‚’CSVã«åæ˜ ã§ãã¾ã™ï¼ˆæ—¢å­˜åˆ—ã¯ç¶­æŒã—ã¤ã¤ã€ç‰©ä»¶åã‚’è¿½åŠ ï¼‰ã€‚
â€»ä¿å­˜ã¯ BOM ä»˜ãUTF-8ã®ã¾ã¾ã€‚

1) suumo_mansion_review.pyï¼ˆå®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã ã‘è¿½è¨˜ï¼‰
ã€Œmetaã«ç‰©ä»¶åãŒãªã‘ã‚Œã° title ã‚’å…¥ã‚Œã‚‹ã€ä¿é™ºã‚’å…¥ã‚Œã¦ãŠãã¾ã™ï¼ˆæ—¢ã«å–å¾—å®Ÿè£…æ¸ˆã¿ã§ã™ãŒã€ç©ºè½ã¡å¯¾ç­–ï¼‰ã€‚

# ... è¿”å´ç›´å‰ã€metaã‚’çµ„ã‚€ç®‡æ‰€ã§:

meta = {
    "ç‰©ä»¶å": name or title,   # â† ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å¿…ãšå…¥ã‚Œã‚‹
    "æ‰€åœ¨åœ°": location,
    "é§…å¾’æ­©": walk,
    "ç¯‰å¹´æœˆ": built,
    "å°‚æœ‰é¢ç©": sqm,
    "é–“å–ã‚Š": layout,
    "è²©å£²ä¾¡æ ¼": price,
    "source": base_url,
}
return {"url": detail_url, "title": name or title, "body": "", "meta": meta}
â€» parse_listing_card_property_unit / parse_listing_card_cassette / parse_detail_page ã®3ã‹æ‰€ã¨ã‚‚åŒæ§˜ã«ã€‚

2) normalize.pyï¼ˆç‰©ä»¶åã‚’æ­£è¦åŒ–ã«è¿½åŠ ï¼‰
æ­£è¦åŒ–é–¢æ•°ã§ ç‰©ä»¶å ã‚’é€šã™ã‚ˆã†ã«ã—ã¾ã™ã€‚
ï¼ˆé–¢æ•°åã‚„æ§‹é€ ã¯ãƒªãƒã‚¸ãƒˆãƒªã«åˆã‚ã›ã¦å·®ã—æ›¿ãˆã¦ãã ã•ã„ã€‚ä¸‹ã¯å…¸å‹ä¾‹ã€‚ï¼‰

# normalize.py

# æ—¢å­˜: å‡ºåŠ›åˆ—ãŒå›ºå®šã•ã‚Œã¦ã„ã‚‹ã¯ãš
# ä¾‹: OUTPUT_COLS = ["url","fetched_at","status","æ‰€åœ¨åœ°","é§…å¾’æ­©","ç¯‰å¹´æœˆ","å°‚æœ‰é¢ç©","é–“å–ã‚Š","è²©å£²ä¾¡æ ¼","source"]
# â†’ ã“ã“ã«ã€Œç‰©ä»¶åã€ã‚’è¿½åŠ 
OUTPUT_COLS = [
    "url", "fetched_at", "status",
    "ç‰©ä»¶å",              # â† è¿½åŠ 
    "æ‰€åœ¨åœ°", "é§…å¾’æ­©", "ç¯‰å¹´æœˆ", "å°‚æœ‰é¢ç©", "é–“å–ã‚Š", "è²©å£²ä¾¡æ ¼", "source"
]

def normalize_item(item: dict) -> dict:
    meta = item.get("meta", {}) or {}

    # ç‰©ä»¶åã¯ meta ãŒå„ªå…ˆã€ç„¡ã‘ã‚Œã° title ã‚’åˆ©ç”¨
    name = meta.get("ç‰©ä»¶å") or item.get("title") or ""

    row = {
        "url": item.get("url", ""),
        "fetched_at": item.get("fetched_at", ""),  # å‘¼ã³å‡ºã—å´ã§åŸ‹ã‚ã¦ã„ã‚‹æƒ³å®š
        "status": item.get("status", ""),
        "ç‰©ä»¶å": name,                             # â† è¿½åŠ 
        "æ‰€åœ¨åœ°": meta.get("æ‰€åœ¨åœ°", ""),
        "é§…å¾’æ­©": meta.get("é§…å¾’æ­©", ""),
        "ç¯‰å¹´æœˆ": meta.get("ç¯‰å¹´æœˆ", ""),
        "å°‚æœ‰é¢ç©": meta.get("å°‚æœ‰é¢ç©", ""),
        "é–“å–ã‚Š": meta.get("é–“å–ã‚Š", ""),
        "è²©å£²ä¾¡æ ¼": meta.get("è²©å£²ä¾¡æ ¼", ""),
        "source": meta.get("source", item.get("source", "")),
    }
    # ä½™è¨ˆãªåˆ—ã¯ã“ã“ã§è½ã¨ã•ã‚Œã‚‹ï¼ˆ= OUTPUT_COLSå¤–ã¯ä¿å­˜ã—ãªã„ï¼‰
    return {k: row.get(k, "") for k in OUTPUT_COLS}
3) pipeline_scrape_suumo.pyï¼ˆæ›¸ãå‡ºã—åˆ—ã«ã€Œç‰©ä»¶åã€ã‚’å«ã‚ã‚‹ï¼‰
CSVãƒ©ã‚¤ã‚¿ãŒ åˆ—åã‚’å›ºå®š ã—ã¦ã„ã‚‹ã®ã§ã€ã“ã“ã«ã‚‚è¿½åŠ ãŒå¿…è¦ã§ã™ã€‚

# pipeline_scrape_suumo.py

from normalize import normalize_item, OUTPUT_COLS
import csv

# ... ã©ã“ã‹ã§ OUTPUT_COLS ã‚’ä½¿ã£ã¦ãƒ˜ãƒƒãƒ€ã‚’æ›¸ã„ã¦ã„ã‚‹ã¯ãš
# ä¾‹:
with open(out_path, "w", encoding="utf-8-sig", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=OUTPUT_COLS, extrasaction="ignore")
    writer.writeheader()
    for item in scraped_items:
        norm = normalize_item(item)  # â† ã“ã“ã§ã€Œç‰©ä»¶åã€å…¥ã‚Šã® dict ã‚’ä½œã‚‹
        writer.writerow(norm)
ãƒã‚¤ãƒ³ãƒˆ

DictWriter ã® fieldnames ãŒåˆ—ã‚’æ±ºã‚ã¾ã™ã€‚ã“ã“ã«ã€Œç‰©ä»¶åã€ãŒãªã„ã¨çµ¶å¯¾ã«å‡ºã¾ã›ã‚“ã€‚

extrasaction="ignore" ã®ã¾ã¾ã§ã‚‚OKï¼ˆæœªçŸ¥åˆ—ã¯æ¨ã¦ã‚‰ã‚Œã‚‹ï¼‰ã€‚

æ—¢å­˜ã®7åˆ—é‹ç”¨ãŒåˆ¥CSVã§å¿…è¦ãªå ´åˆã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã« OUTPUT_COLS ã‚’åˆ†ã‘ã‚‹ï¼ˆä¾‹ï¼šOUTPUT_COLS_SUUMO ã¨ OUTPUT_COLS_REVIEWï¼‰ã®ãŒå®‰å…¨ã§ã™ã€‚

ä»•ä¸ŠãŒã‚Šç¢ºèªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
 suumo_mansion_review.py ã®è¿”å´ meta["ç‰©ä»¶å"] ãŒç©ºã«ãªã‚‰ãªã„ï¼ˆfallbackã‚ã‚Šï¼‰

 normalize.py ã® OUTPUT_COLS ã«ã€Œç‰©ä»¶åã€ãŒå…¥ã£ã¦ã„ã‚‹

 pipeline_scrape_suumo.py ã® DictWriter(fieldnames=OUTPUT_COLS) ãŒã“ã®åˆ—ã‚’å‚ç…§

 å‡ºåŠ›CSVã®ãƒ˜ãƒƒãƒ€ã«ã€Œç‰©ä»¶åã€ãŒç¾ã‚Œã¦ã„ã‚‹

 æ–‡å­—ã‚³ãƒ¼ãƒ‰ã¯ encoding="utf-8-sig" ã®ã¾ã¾

ã‚‚ã— åˆ¥ã®CSVï¼ˆä¾‹ï¼šãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ï¼‰ã‚’7åˆ—å›ºå®šã§ç¶­æŒã—ãŸã„å ´åˆã¯ã€åˆ—ã‚»ãƒƒãƒˆã‚’åˆ†é›¢ã—ã¾ã™ã€‚ä¾‹ï¼š

# normalize.py
OUTPUT_COLS_SUUMO = ["url","fetched_at","status","ç‰©ä»¶å","æ‰€åœ¨åœ°","é§…å¾’æ­©","ç¯‰å¹´æœˆ","å°‚æœ‰é¢ç©","é–“å–ã‚Š","è²©å£²ä¾¡æ ¼","source"]
OUTPUT_COLS_REVIEW = ["url","fetched_at","status","location","station_walk","built_ym","exclusive_area_sqm","layout","price_jpy"]  # æ—¢å­˜7åˆ—

def normalize_suumo(item): ...  # ç‰©ä»¶åã‚ã‚Š
def normalize_review(item): ... # 7åˆ—å›ºå®š
pipeline_scrape_suumo.py å´ã§ã©ã¡ã‚‰ã‚’ä½¿ã†ã‹é¸ã¹ã°ã€æ—¢å­˜ãƒ•ãƒ­ãƒ¼ã‚’å£Šã•ãšã«ã€Œç‰©ä»¶åã€ä»˜ãCSVã‚’æ–°è¦ã§å‡ºã›ã¾ã™ã€‚

å¿…è¦ãªã‚‰ã€ã‚ãªãŸã®ãƒªãƒã‚¸ãƒˆãƒªæ§‹æˆã«åˆã‚ã›ã¦å®Ÿãƒ•ã‚¡ã‚¤ãƒ«åãƒ»é–¢æ•°åã§æ›¸ãç›´ã—ãŸå®Œå…¨ãƒ‘ãƒƒãƒã‚‚ã¾ã¨ã‚ã¾ã™ã€‚

ã‚ãªãŸ:
suumo_mansion_review.py ã®ãƒ‡ãƒãƒƒã‚°ã®ãŸã‚ã€ç›´æ¥å®Ÿè¡Œã—ãŸã„ã§ã™ã€‚

if __name__ == "__main__":
    # url_list.txt ã‹ã‚‰èª­ã¿è¾¼ã‚“ã§å…¨ä»¶ã‚¯ãƒ­ãƒ¼ãƒ« â†’ ä»¶æ•°ã¨ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
    url_list_path = os.getenv("URL_LIST", "url_list.txt")
    urls = []
    if os.path.exists(url_list_path):
        with open(url_list_path, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip()]
    urls = ["https://suumo.jp/ms/chuko/tokyo/sc_chuo/?page=3"]
    all_items = []
    for u in urls:
        try:
            items = crawl_list(u)
            all_items.extend(items)
            print(f"[OK] {u} -> {len(items)} items")
        except Exception as e:
            print(f"[ERR] {u}: {e}")

    print(f"TOTAL: {len(all_items)} items")
    if all_items:
        print(all_items[0])

[OK] https://suumo.jp/ms/chuko/tokyo/sc_chuo/?page=3 -> 40 items
TOTAL: 40 items
{'url': 'https://suumo.jp/ms/chuko/tokyo/sc_chuo/nc_78229644/', 'title': 'ã‚³ã‚¹ãƒ¢ã‚°ãƒ©ã‚·ã‚¢äººå½¢ç”º', 'body': '', 'meta': {'ç‰©ä»¶å': 'ã‚³ã‚¹ãƒ¢ã‚°ãƒ©ã‚·ã‚¢äººå½¢ç”º', 'æ‰€åœ¨åœ°': 'æ±äº¬éƒ½ä¸­å¤®åŒºæ—¥æœ¬æ©‹å¯Œæ²¢ç”º', 'é§…å¾’æ­©': 'å¾’æ­©3åˆ†', 'ç¯‰å¹´æœˆ': '2009å¹´1æœˆ', 'å°‚æœ‰é¢ç©': '40.71ã¡', 'é–“å–ã‚Š': '1LDK', 'è²©å£²ä¾¡æ ¼': '9299ä¸‡å††', 'source': 'https://suumo.jp/ms/chuko/tokyo/sc_chuo/?page=3'}}
[Finished in 4.1s]

ç‰©ä»¶åã¯ã¡ã‚ƒã‚“ã¨ç¾ã‚Œã¾ã™ã€‚

parse_detail_pageã€€ã®å®Ÿè¡Œã«ã‚ˆã‚Šã€åé›†ã•ã‚Œã¾ã™ã€‚

ã—ã‹ã—ã€storage.py ã«ã€ã•ã¾ã–ã¾ãªé–¢æ•°ã«ã€ç‰©ä»¶åãŒå…¥ã£ã¦ã„ãªã„ã§ã™ã€‚

# storage.py â€” CSVã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆprefixä»˜ããƒ•ã‚¡ã‚¤ãƒ«å & BOMä»˜ãUTF-8ï¼‰
from __future__ import annotations
from pathlib import Path
from datetime import datetime
import csv
from typing import Dict, List, Optional

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

def _make_csv_path() -> Path:
    """ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ã£ã¦ prefixä»˜ããƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ"""
    ts = datetime.now().strftime("%Y%m%d%H%M")
    return DATA_DIR / f"{ts}-suumo_reviews.csv"

# å®Ÿè¡Œã”ã¨ã«æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã†
CSV_PATH = _make_csv_path()

# æ®‹ã™åˆ—
COLUMNS = [
    "url",
    "fetched_at",
    "status",
    "location",
    "station_walk",
    "built_ym",
    "exclusive_area_sqm",
    "layout",
    "price_jpy",
]

# ---------- åŸºæœ¬I/O ----------

def _ensure_csv():
    """CSVãŒãªã‘ã‚Œã°æ–°è¦ä½œæˆ"""
    if not CSV_PATH.exists():
        with CSV_PATH.open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(f, fieldnames=COLUMNS)
            w.writeheader()

def _load_all() -> List[Dict[str, str]]:
    _ensure_csv()
    rows: List[Dict[str, str]] = []
    with CSV_PATH.open("r", newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for r in reader:
            row = {col: r.get(col, "") for col in COLUMNS}
            if (row.get("url") or "").strip():  # urlç©ºã¯é™¤å¤–
                rows.append(row)
    return rows

def _write_all(rows: List[Dict[str, str]]):
    tmp = CSV_PATH.with_suffix(".tmp")
    with tmp.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=COLUMNS)
        w.writeheader()
        for r in rows:
            row = {col: r.get(col, "") for col in COLUMNS}
            if (row.get("url") or "").strip():
                w.writerow(row)
    tmp.replace(CSV_PATH)

def _now_iso_utc() -> str:
    return datetime.utcnow().isoformat()

# ---------- meta å±•é–‹ ----------

def _extract_meta(meta: dict) -> Dict[str, str]:
    def pick(*keys, default=""):
        for k in keys:
            if k in meta and meta[k] is not None:
                v = str(meta[k]).strip()
                if v:
                    return v
        return default

    return {
        "location": pick("location", "æ‰€åœ¨åœ°", "ä½æ‰€"),
        "station_walk": pick("station_walk", "é§…å¾’æ­©", "æ²¿ç·šãƒ»é§…"),
        "built_ym": pick("built_ym", "ç¯‰å¹´æœˆ", "ç¯‰å¹´"),
        "exclusive_area_sqm": pick("exclusive_area_sqm", "å°‚æœ‰é¢ç©", "é¢ç©"),
        "layout": pick("layout", "é–“å–ã‚Š"),
        "price_jpy": pick("price_jpy", "è²©å£²ä¾¡æ ¼", "ä¾¡æ ¼", "price"),
    }

# ---------- å…¬é–‹API ----------

def get_db():
    _ensure_csv()
    return str(CSV_PATH)

def upsert_review(conn, url: str, status: int, title: str, body: str, meta: dict):
    if not url or not str(url).strip():
        return

    rows = _load_all()
    idx: Optional[int] = None
    for i, r in enumerate(rows):
        if r.get("url") == url:
            idx = i
            break

    meta_ex = _extract_meta(meta or {})

    rec = {
        "url": url.strip(),
        "fetched_at": _now_iso_utc(),
        "status": str(status),
        "location": meta_ex["location"],
        "station_walk": meta_ex["station_walk"],
        "built_ym": meta_ex["built_ym"],
        "exclusive_area_sqm": meta_ex["exclusive_area_sqm"],
        "layout": meta_ex["layout"],
        "price_jpy": meta_ex["price_jpy"],
    }

    if idx is None:
        rows.append(rec)
    else:
        rows[idx] = rec

    _write_all(rows)

def read_all_reviews() -> List[Dict[str, str]]:
    return _load_all()

def get_by_url(query_url: str) -> Optional[Dict[str, str]]:
    for r in _load_all():
        if r.get("url") == query_url:
            return r
    return None


ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
ChatGPT:

æ€è€ƒæ™‚é–“: 8s
ãƒŠã‚¤ã‚¹åˆ‡ã‚Šåˆ†ã‘ã§ã™ã€‚storage.py å´ã§åˆ—ã‚’å›ºå®šã—ã¦ã„ã‚‹ã®ãŒåŸå› ãªã®ã§ã€ã€Œç‰©ä»¶åã€ã‚’æ­£å¼ã‚«ãƒ©ãƒ ã«è¿½åŠ ã—ã€metaâ†’å†…éƒ¨åˆ—ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼†ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆtitleï¼‰ã¾ã§å…¥ã‚ŒãŸç‰ˆã‚’ç”¨æ„ã—ã¾ã—ãŸã€‚ã“ã‚Œã«å·®ã—æ›¿ãˆã¦ãã ã•ã„ã€‚

storage.pyï¼ˆä¸Šæ›¸ãç”¨ãƒ»å®Œå…¨ç‰ˆï¼‰
# storage.py â€” CSVã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆprefixä»˜ããƒ•ã‚¡ã‚¤ãƒ«å & BOMä»˜ãUTF-8ï¼‰
from __future__ import annotations
from pathlib import Path
from datetime import datetime
import csv
from typing import Dict, List, Optional

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

def _make_csv_path() -> Path:
    """ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ã£ã¦ prefixä»˜ããƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ"""
    ts = datetime.now().strftime("%Y%m%d%H%M")
    return DATA_DIR / f"{ts}-suumo_reviews.csv"

# å®Ÿè¡Œã”ã¨ã«æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã†
CSV_PATH = _make_csv_path()

# æ®‹ã™åˆ—ï¼ˆâ˜… ç‰©ä»¶åã‚’è¿½åŠ ï¼‰
COLUMNS = [
    "url",
    "fetched_at",
    "status",
    "property_name",         # â† è¿½åŠ ï¼ˆå†…éƒ¨åï¼‰
    "location",
    "station_walk",
    "built_ym",
    "exclusive_area_sqm",
    "layout",
    "price_jpy",
]

# ---------- åŸºæœ¬I/O ----------

def _ensure_csv():
    """CSVãŒãªã‘ã‚Œã°æ–°è¦ä½œæˆ"""
    if not CSV_PATH.exists():
        with CSV_PATH.open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(f, fieldnames=COLUMNS, extrasaction="ignore")
            w.writeheader()

def _load_all() -> List[Dict[str, str]]:
    _ensure_csv()
    rows: List[Dict[str, str]] = []
    with CSV_PATH.open("r", newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—§ã‚¹ã‚­ãƒ¼ãƒã§ã‚‚å®‰å…¨ã«èª­ã‚€
        for r in reader:
            row = {col: r.get(col, "") for col in COLUMNS}
            if (row.get("url") or "").strip():  # urlç©ºã¯é™¤å¤–
                rows.append(row)
    return rows

def _write_all(rows: List[Dict[str, str]]):
    tmp = CSV_PATH.with_suffix(".tmp")
    with tmp.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=COLUMNS, extrasaction="ignore")
        w.writeheader()
        for r in rows:
            row = {col: r.get(col, "") for col in COLUMNS}
            if (row.get("url") or "").strip():
                w.writerow(row)
    tmp.replace(CSV_PATH)

def _now_iso_utc() -> str:
    return datetime.utcnow().isoformat()

# ---------- meta å±•é–‹ ----------

def _extract_meta(meta: dict, title: str = "") -> Dict[str, str]:
    """meta ã‹ã‚‰å†…éƒ¨åˆ—ç”¨ã«å€¤ã‚’æŠ½å‡ºã€‚ãƒ©ãƒ™ãƒ«æºã‚Œã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«å¯¾å¿œã€‚"""
    def pick(keys, default=""):
        for k in keys:
            if k in meta and meta[k] is not None:
                v = str(meta[k]).strip()
                if v:
                    return v
        return default

    # ç‰©ä»¶åã®ãƒ©ãƒ™ãƒ«æºã‚Œã‚’å¸åï¼ˆæœ€å¾Œã«titleãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    property_name = pick(["ç‰©ä»¶å", "ãƒãƒ³ã‚·ãƒ§ãƒ³å", "å»ºç‰©å", "åç§°"])
    if not property_name:
        property_name = (title or "").strip()

    return {
        "property_name": property_name,
        "location":      pick(["location", "æ‰€åœ¨åœ°", "ä½æ‰€"]),
        "station_walk":  pick(["station_walk", "é§…å¾’æ­©", "æ²¿ç·šãƒ»é§…"]),
        "built_ym":      pick(["built_ym", "ç¯‰å¹´æœˆ", "ç¯‰å¹´"]),
        "exclusive_area_sqm": pick(["exclusive_area_sqm", "å°‚æœ‰é¢ç©", "é¢ç©"]),
        "layout":        pick(["layout", "é–“å–ã‚Š"]),
        "price_jpy":     pick(["price_jpy", "è²©å£²ä¾¡æ ¼", "ä¾¡æ ¼", "price"]),
    }

# ---------- å…¬é–‹API ----------

def get_db():
    _ensure_csv()
    return str(CSV_PATH)

def upsert_review(conn, url: str, status: int, title: str, body: str, meta: dict):
    if not url or not str(url).strip():
        return

    rows = _load_all()
    idx: Optional[int] = None
    for i, r in enumerate(rows):
        if r.get("url") == url:
            idx = i
            break

    meta_ex = _extract_meta(meta or {}, title=title)

    rec = {
        "url": url.strip(),
        "fetched_at": _now_iso_utc(),
        "status": str(status),
        "property_name": meta_ex["property_name"],     # â† è¿½åŠ 
        "location": meta_ex["location"],
        "station_walk": meta_ex["station_walk"],
        "built_ym": meta_ex["built_ym"],
        "exclusive_area_sqm": meta_ex["exclusive_area_sqm"],
        "layout": meta_ex["layout"],
        "price_jpy": meta_ex["price_jpy"],
    }

    if idx is None:
        rows.append(rec)
    else:
        rows[idx] = rec

    _write_all(rows)

def read_all_reviews() -> List[Dict[str, str]]:
    return _load_all()

def get_by_url(query_url: str) -> Optional[Dict[str, str]]:
    for r in _load_all():
        if r.get("url") == query_url:
            return r
    return None
è£œè¶³
åˆ—ã‚¹ã‚­ãƒ¼ãƒãŒå¤‰ã‚ã‚‹ã¨æ—¢å­˜CSVã¨ã¯ãƒ˜ãƒƒãƒ€ãŒç•°ãªã‚‹ãŸã‚ã€åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ã—ãªã„è¨­è¨ˆã«ã—ã¦ã„ã¾ã™ï¼ˆæœ¬ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ¯å›æ–°ã—ã„ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®CSVã‚’ä½œæˆï¼‰ã€‚

upsert_review() å‘¼ã³å‡ºã—å´ï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰ã«å¤‰æ›´ã¯ä¸è¦ã§ã™ã€‚title ã‚‚æ—¢ã«æ¸¡ã•ã‚Œã¦ã„ã¾ã™ã—ã€meta["ç‰©ä»¶å"] ãŒç„¡ã„å ´åˆã¯ title ã‚’è‡ªå‹•æ¡ç”¨ã—ã¾ã™ã€‚

æ—§CSVã‚’é–‹ãã¨ property_name åˆ—ãŒç©ºã«è¦‹ãˆã¾ã™ãŒã€æ–°ã—ãç”Ÿæˆã•ã‚Œã‚‹CSVã«ã¯å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

ã“ã®çŠ¶æ…‹ã§å†å®Ÿè¡Œã™ã‚Œã°ã€property_nameï¼ˆç‰©ä»¶åï¼‰ãŒCSVã«å…¥ã‚Šã¾ã™ã€‚ã‚‚ã—è‹±èªåˆ—ã§ã¯ãªãæ—¥æœ¬èªåˆ—åã§å‡ºã—ãŸã„å ´åˆã¯ã€COLUMNS ã¨ _extract_meta ã®ã‚­ãƒ¼åã‚’ ç‰©ä»¶å ã«æ›¿ãˆã‚‹ã ã‘ã§OKã§ã™ã€‚

ã‚ãªãŸ:
ä½¿ãˆã¾ã™ã€‚ãªãŠã€VPNä½¿ç”¨ã§ã™ãŒã€ãã®ã¾ã¾ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ã¦ã‚‚å•é¡Œãªã„ã‹çŸ¥ã‚ŠãŸã„ã§ã™ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®‰å®šå®‰å…¨ã«ã—ãŸã„ã§ã™ã€‚
ChatGPT:
VPNã‚’ä½¿ã£ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã“ã¨è‡ªä½“ã¯ã€å¿…ãšã—ã‚‚é•æ³•ã¨ã„ã†ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒªã‚¹ã‚¯ã¨æ³¨æ„ç‚¹ãŒã‹ãªã‚Šã‚ã‚Šã¾ã™ã€‚ä¼æ¥­ã§å®‰å®šå®‰å…¨ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ãƒ­ã‚¸ãƒƒã‚¯ã‚’é‹ç”¨ã™ã‚‹ãªã‚‰ã€ä»¥ä¸‹ã‚’ç¢ºèªãƒ»è¨­è¨ˆã—ã¦ãŠãã®ãŒãƒ™ã‚¹ãƒˆã§ã™ã€‚

âš ï¸ ä¸»ãªãƒªã‚¹ã‚¯
ãƒªã‚¹ã‚¯	å†…å®¹
åˆ©ç”¨è¦ç´„é•å	ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã‚µã‚¤ãƒˆï¼ˆSUUMOãªã©ï¼‰ã®è¦ç´„ãŒã€Œè‡ªå‹•å–å¾—ã‚’ç¦æ­¢ã€ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã€‚è¦ç´„é•åã¯å¥‘ç´„ä¸Šãƒ»æ°‘äº‹ä¸Šã®å•é¡Œã«ãªã‚‹å¯èƒ½æ€§ã‚ã‚Šã€‚ 
ãƒã‚¦ã‚¹ã‚±ã‚¢ãƒ©ãƒœ
+2
é¢å€’ãªå˜ç´”ä½œæ¥­ã‚’è‡ªå‹•åŒ–ã—ã€ ç¹°ã‚Šè¿”ã—ã‹ã‚‰è§£æ”¾ã™ã‚‹RPAãƒ„ãƒ¼ãƒ«ã€ŒBizRobo!ã€
+2
è‘—ä½œæ¨©æ³•ã®å•é¡Œ	å–å¾—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè‘—ä½œç‰©ã‹ã©ã†ã‹ã€ã‚ã‚‹ã„ã¯ãƒ‘ãƒ–ãƒªãƒƒã‚¯æƒ…å ±ã‹ãªã©ãŒå•ã‚ã‚Œã‚‹ã€‚è¨€è‘‰é£ã„ã‚„èª¬æ˜ã€å†™çœŸç­‰ã¯è‘—ä½œæ¨©ã§ä¿è­·ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚ã‚Šã€‚ 
Last Data Inc. ï½œ ã™ã¹ã¦ã®äººã«æˆé•·ã‚’
+1
ä¸æ­£ã‚¢ã‚¯ã‚»ã‚¹ç¦æ­¢æ³•	ãƒ­ã‚°ã‚¤ãƒ³ã‚„ä¼šå“¡å°‚ç”¨ãƒšãƒ¼ã‚¸ã€èªè¨¼ä»˜ãã®ãƒšãƒ¼ã‚¸ã‚’çªç ´ã—ã¦æƒ…å ±å–å¾—ã™ã‚‹ã“ã¨ã¯é•æ³•ã«ãªã‚‹ã“ã¨ã‚‚ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒã€Œéå…¬é–‹éƒ¨åˆ†ã€ã«è§¦ã‚Œã‚‹ã¨ãƒªã‚¹ã‚¯ãŒã‚ã‚‹ã€‚ 
ãƒã‚¦ã‚¹ã‚±ã‚¢ãƒ©ãƒœ
+1
ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦ / IPãƒ–ãƒ­ãƒƒã‚¯	å¤šæ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚„ä¸è‡ªç„¶ãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã‚µãƒ¼ãƒãƒ¼å´ã«æ¤œçŸ¥ã•ã‚Œã€IPæ‹’å¦ã€CAPTCHAå°å…¥ãªã©ã•ã‚Œã‚‹ã€‚VPNã‚’ä½¿ã£ã¦IPã‚’éš ã—ã¦ã‚‚ã€ç¹°ã‚Šè¿”ã—ã‚„åŒä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã€‚ 
æ ªå¼ä¼šç¤¾ã‚·ãƒ«ã‚¯ãƒ»ãƒ©ãƒœãƒ©ãƒˆãƒª
+1
ä¿¡é ¼ãƒ»å€«ç†ãƒ»ä¿¡ç”¨ã®å•é¡Œ	é¡§å®¢ã‚„å–å¼•å…ˆã‹ã‚‰ã€Œæ­£å½“æ€§ã¯ï¼Ÿã€ã¨å•ã‚ã‚Œã‚‹å¯èƒ½æ€§ã€‚ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãŒéå…¬å¼ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«é ¼ã‚‹ã¨ã€ãƒ–ãƒ©ãƒ³ãƒ‰ãƒªã‚¹ã‚¯ã«ãªã‚‹ã“ã¨ã‚‚ã€‚
âœ… å®‰å®šå®‰å…¨ã«ã™ã‚‹ãŸã‚ã®è¨­è¨ˆãƒ»é‹ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
VPNã‚’ä½¿ã£ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®‰å®šãƒ»å®‰å…¨ã«ã™ã‚‹ãŸã‚ã«ã€å…·ä½“çš„ã«ä»¥ä¸‹ã®ã‚ˆã†ãªè¨­è¨ˆãƒ»é‹ç”¨ã‚’ã™ã‚‹ã®ãŒæœ›ã¾ã—ã„ã§ã™ã€‚

å¯¾ç­–	å†…å®¹
åˆ©ç”¨è¦ç´„ã‚’ç¢ºèª	å¯¾è±¡ã®ã‚µã‚¤ãƒˆï¼ˆSUUMOãªã©ï¼‰ã® Terms of Service / åˆ©ç”¨è¦ç´„ã‚’èª­ã¿ã€ã€Œè‡ªå‹•å–å¾—ã€ã€Œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã€ã®è¨˜è¼‰ãŒãªã„ã‹ã€è¨±å¯ã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã€‚å¿…è¦ãªã‚‰ã‚µã‚¤ãƒˆé‹å–¶è€…ã«å•ã„åˆã‚ã›ã‚‹ã€‚
robots.txt ã‚’ãƒã‚§ãƒƒã‚¯	robots.txt ã«æ˜ç¤ºçš„ã«ã‚¢ã‚¯ã‚»ã‚¹ã‚’ç¦æ­¢ã—ã¦ã„ã‚‹ãƒ‘ã‚¹ãŒãªã„ã‹ç¢ºèªã™ã‚‹ã€‚æŠ€è¡“çš„ã«â€œå®ˆã‚‹ã¹ãæŒ‡é‡â€ã¨ã—ã¦ã€‚ 
Last Data Inc. ï½œ ã™ã¹ã¦ã®äººã«æˆé•·ã‚’
+1
ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ã‚’è¨­ã‘ã‚‹	REQUEST_INTERVAL ã‚’é©åˆ‡ã«å–ã‚Šã€ä¹±æš´ãªã‚¢ã‚¯ã‚»ã‚¹ã‚’ã—ãªã„ã€‚åˆ†æ•£ãƒ»ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã™ã‚‹ç­‰ã€ã€Œäººé–“ã‚‰ã—ã„ã€æŒ™å‹•ã‚’æ¨¡å€£ã™ã‚‹ã€‚
IPãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ãƒ—ãƒ­ã‚­ã‚·åˆ©ç”¨	ä¸€ã¤ã®IPã‹ã‚‰å¤šæ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã™ã‚‹ã®ã‚’é¿ã‘ã‚‹ã€‚VPNã‚’ä½¿ã†ãªã‚‰ IP ã‚’å›ºå®šã—ã™ããªã„ã€‚è¤‡æ•°ã®VPNã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚„ä¿¡é ¼ã§ãã‚‹ãƒ—ãƒ­ã‚­ã‚·ã‚’åˆ©ç”¨ã€‚
User-Agent, Headers ã®è¨­å®š	é©åˆ‡ãª User-Agent ã‚’ä½¿ã„ã€å¿…è¦ãªã‚‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è‡ªç„¶ãªã‚‚ã®ã«ã™ã‚‹ã€‚ã‚ã¾ã‚Šã«ã‚‚ bot ã£ã½ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã—ãªã„ã€‚
å¤±æ•—æ™‚ã®ãƒªãƒˆãƒ©ã‚¤ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°	ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒé200ã®æ™‚ã€ã¾ãŸã¯ HTMLæ§‹é€ ãŒå¤‰ã‚ã£ãŸæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¨­ã‘ã‚‹ã€‚å¼·ããƒ†ãƒ³ãƒãƒ©ãƒ«ãªã‚‚ã®ï¼ˆJSã®èª­ã¿è¾¼ã¿ãªã©ï¼‰ã«ä¾å­˜ã—ã™ããªã„è¨­è¨ˆã€‚
ãƒ­ã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°	ã„ã¤ã©ã®URLã‚’å–ã£ãŸã‹ã€IPã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ»ã‚¨ãƒ©ãƒ¼å›æ•°ãªã©ã‚’ãƒ­ã‚°ã™ã‚‹ã€‚ã‚‚ã—ãƒ–ãƒ­ãƒƒã‚¯ã‚„å•ã„åˆã‚ã›ãŒæ¥ãŸã¨ãã«é€Ÿã‚„ã‹ã«å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«ã€‚
ãƒ‡ãƒ¼ã‚¿ã®æœ€å°å–å¾—ãƒ»å¿…è¦æƒ…å ±ã®é™å®š	è«å¤§ãªãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚‹ã®ã§ã¯ãªãã€æœ¬å½“ã«å¿…è¦ãªæƒ…å ±ã ã‘ã‚’å–å¾—ã€‚ç”»åƒã‚„é«˜å®¹é‡ãªãƒªã‚½ãƒ¼ã‚¹ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãªã„ã‚ˆã†ã«ã€‚
æ³•å‹™ãƒã‚§ãƒƒã‚¯	ç‰¹ã«å•†ç”¨ç›®çš„ãªã‚‰ã€ç¤¾å†…æ³•å‹™ã¾ãŸã¯å¤–éƒ¨å¼è­·å£«ã«ã€Œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã€ã€Œå–å¾—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã€ã€Œåˆ©ç”¨æ–¹æ³•ã€ã®åˆæ³•æ€§ã‚’ç¢ºèªã—ã¦ãŠãã€‚
ğŸ” VPNã‚’ä½¿ã†ã“ã¨è‡ªä½“ã®æ³•çš„ãªå•é¡Œ
æ—¥æœ¬å›½å†…ã§ã‚ã‚Œã°ã€VPN ã‚’ä½¿ã†ã“ã¨è‡ªä½“ã¯åˆæ³•ã§ã™ã€‚é€šä¿¡ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ãªã©ã®æ‰‹æ®µã¨ã—ã¦æ™®é€šã«ä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚ 
Cyber Gorillaï¼ˆã‚µã‚¤ãƒãƒ¼ã‚´ãƒªãƒ©ï¼‰
+1

ãŸã ã—ã€VPN ã§ã‚¢ã‚¯ã‚»ã‚¹å…ƒã‚’å½ã‚‹ã“ã¨ã§ã€ã€Œåˆ©ç”¨è¦ç´„é•åã€ã‚’ã•ã‚‰ã«é‡ãè¦‹ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã€è¦ç´„ã«ã€ŒIPåˆ¶é™ãƒ»ã‚¢ã‚¯ã‚»ã‚¹å…ƒã®åˆ¶ç´„ã€ãŒã‚ã‚‹ã‹ã©ã†ã‹ã®ç¢ºèªãŒé‡è¦ã€‚

ã¾ãŸã€åˆ©ç”¨è€…ãŒ IP ã‚’å½è£…ã—ã¦ä½•ã‹æ‚ªæ„ãƒ»ä¸æ­£ãªè¡Œç‚ºã‚’ã™ã‚‹ãªã‚‰ã€ãã‚Œã¯åˆ¥ã®å•é¡Œã§ã™ã€‚

ã‚‚ã—ã‚ˆã‘ã‚Œã°ã€SUUMOã®åˆ©ç”¨è¦ç´„ã‚’åƒ•ã®ã»ã†ã§èª­ã¿è¾¼ã‚“ã§ã€ã€Œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ˜ç¤ºç¦æ­¢ã‹ã©ã†ã‹ã€ã€ŒVPNåˆ©ç”¨ã«é–¢é€£ã™ã‚‹è¦ç´„ã€ãŒã‚ã‚‹ã‹ã‚’èª¿ã¹ã¦ãŠä¼ãˆã—ã¾ã™ã‹ï¼Ÿ

ã‚ãªãŸ:
https://github.com/britishbluecat/reconstruction_LLM/tree/dev_add_building_name

ä¸Šè¨˜ã€branchã«ã¦ã€ç‰©ä»¶åãŒåæ˜ ã•ã‚Œã¾ã™ã€‚

ã“ã‚Œã‚’ã€dev branch ã«çµ±åˆã™ã‚Œã°æ­£ã—ã„ã§ã™ã‹ã€‚
ChatGPT:

æ€è€ƒæ™‚é–“: æ•°ç§’
ã¯ã„ã€ãã®ãƒ–ãƒ©ãƒ³ãƒã§ã€Œç‰©ä»¶åã€ãŒCSVã¾ã§å‡ºã¦ã„ã‚‹ãªã‚‰ã€dev ã«çµ±åˆã§OKã§ã™ã€‚
ãŸã ã€å¾Œæ–¹äº’æ›ï¼ˆæ—¢å­˜ã®7åˆ—CSVï¼‰ã«å½±éŸ¿ãŒå‡ºãªã„ã‚ˆã†ã«ã€ä»¥ä¸‹ã ã‘ãƒã‚§ãƒƒã‚¯ã—ã¦ã‹ã‚‰ãƒãƒ¼ã‚¸ã—ã¦ãã ã•ã„ã€‚

ãƒãƒ¼ã‚¸å‰ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€é‡è¦ï¼‰
ä¿å­˜ã‚¹ã‚­ãƒ¼ãƒç¢ºèª

storage.py ã® COLUMNS ã« property_nameï¼ˆã¾ãŸã¯ ç‰©ä»¶åï¼‰ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã‹

æ—¢å­˜ã®7åˆ—CSVã‚’ä½¿ã†å‡¦ç†ãŒä»–ã«ã‚‚ã‚ã‚‹å ´åˆã¯ã€åˆ—ã‚»ãƒƒãƒˆã‚’åˆ†ã‘ã‚‹ï¼ˆä¾‹ï¼šOUTPUT_COLS_SUUMO ã¨ OUTPUT_COLS_REVIEWï¼‰ã‹ã€ãƒ•ãƒ©ã‚°ã§åˆ‡æ›¿

æ—§CSVã‚’èª­ã¿è¾¼ã‚€ã‚³ãƒ¼ãƒ‰ãŒæ–°åˆ—ãªã—ã§ã‚‚å‹•ãï¼ˆæ¬ æã¯ç©ºã§èª­ã‚€ï¼‰ã“ã¨ã‚’ç¢ºèª

æ­£è¦åŒ–ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

normalize.py ã§ meta["ç‰©ä»¶å"] ãŒç©ºãªã‚‰ title ã‚’å…¥ã‚Œã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒã‚ã‚‹ã‹

ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¥ç¶š

pipeline_scrape_suumo.py ã® DictWriter(fieldnames=...) ã«æ–°åˆ—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹

æ–‡å­—ã‚³ãƒ¼ãƒ‰ã¯ utf-8-sig ã®ã¾ã¾

å®Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼

suumo_mansion_review.py ã‚’å˜ä½“å®Ÿè¡Œã—ã¦ 2ã€œ3URLã§ ä»¶æ•°ã¨ã‚µãƒ³ãƒ—ãƒ«ã‚’ç›®è¦–

read_all_reviews() ã§CSVã‚’èª­ã¿ã€property_name ãŒåŸ‹ã¾ã£ã¦ã„ã‚‹ã‹ç¢ºèª

ãƒãƒ¼ã‚¸æ‰‹é †ï¼ˆä¾‹ï¼‰
git checkout dev
git pull origin dev

git checkout dev_add_building_name
git pull origin dev_add_building_name

# dev ã«ãƒãƒ¼ã‚¸ï¼ˆã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆãŒã‚ã‚Œã°è§£æ¶ˆï¼‰
git checkout dev
git merge --no-ff dev_add_building_name

# å‹•ä½œç¢ºèª
python suumo_mansion_review.py  # å˜ä½“ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ
python pipeline_scrape_suumo.py # CSVå‡ºåŠ›ã¾ã§

# å•é¡Œãªã‘ã‚Œã°ãƒ—ãƒƒã‚·ãƒ¥
git push origin dev
ã‚‚ã—7åˆ—CSVã‚’ç¶­æŒã—ãŸã„å ´åˆï¼ˆå¾Œæ–¹äº’æ›ï¼‰
åˆ—ã‚»ãƒƒãƒˆã‚’åˆ†ã‘ã‚‹ï¼š

OUTPUT_COLS_SUUMO = [..., "property_name", ...]

OUTPUT_COLS_REVIEW = ["url","fetched_at","status","location","station_walk","built_ym","exclusive_area_sqm","layout","price_jpy"]

ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å´ã§ã©ã¡ã‚‰ã‚’å‡ºã™ã‹é¸æŠï¼ˆå¼•æ•°ã‚„ç’°å¢ƒå¤‰æ•° CSV_SCHEMA=suumo/reviewï¼‰

PRãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆç°¡æ˜“ï¼‰
 ç‰©ä»¶å ãŒ meta â†’ normalize â†’ CSV ã¾ã§æµã‚Œã‚‹

 æ—¢å­˜ã®èª­ã¿è¾¼ã¿å‡¦ç†ãŒå£Šã‚Œã¦ã„ãªã„

 ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä»¶æ•°ãŒæƒ³å®šé€šã‚Šï¼ˆpc=100 + ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰

 robots.txt/ToS ã‚’å°Šé‡ã™ã‚‹ãƒ¬ãƒ¼ãƒˆãƒ»ãƒ˜ãƒƒãƒ€ã§ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆREQUEST_INTERVALã€UAï¼‰

 å¤‰æ›´ç‚¹ã‚’ CHANGELOG.md or chatgpt_log/ ã«è¨˜éŒ²

çµè«–ï¼šã“ã®ãƒ–ãƒ©ãƒ³ãƒã®å†…å®¹ã‚’ dev ã«ãƒãƒ¼ã‚¸ã§æ­£ã—ã„ã§ã™ã€‚
ä¸Šã®ãƒã‚§ãƒƒã‚¯ã ã‘æ¸ˆã¾ã›ã¦ã€é€²ã‚ã¡ã‚ƒã£ã¦OKã§ã™ï¼

ã‚ãªãŸ:
ä¸Šè¨˜ã€github ãƒ•ãƒ­ãƒ¼ã§ã™ãŒã€è‡ªåˆ†ã®ãƒ‘ã‚½ã‚³ãƒ³ã§GITHUBãŒãªã„å ´åˆã€ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæ“ä½œã‚’æ•™ãˆã¦ãã ã•ã„ã€‚
ChatGPT:
äº†è§£ã—ã¾ã—ãŸ ğŸ‘
ãƒ­ãƒ¼ã‚«ãƒ«ã« GitHub CLI ã‚„ Git ã‚³ãƒãƒ³ãƒ‰ãŒç„¡ã„å ´åˆã§ã‚‚ã€**GitHub ã®ã‚¦ã‚§ãƒ–UIã ã‘ã§ã€Œãƒ–ãƒ©ãƒ³ãƒçµ±åˆã€**ã¯å¯èƒ½ã§ã™ã€‚

ğŸŒ GitHub ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã§ã®ãƒ–ãƒ©ãƒ³ãƒçµ±åˆæ‰‹é †
1. ãƒªãƒã‚¸ãƒˆãƒªã‚’é–‹ã
ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒªãƒã‚¸ãƒˆãƒªã‚’é–‹ãã¾ã™
ğŸ‘‰ https://github.com/britishbluecat/reconstruction_LLM

2. ãƒ–ãƒ©ãƒ³ãƒã‚’ç¢ºèª
ç”»é¢å·¦ä¸Šã® ã€Œmainã€ã‚„ã€Œdevã€ãªã©ã®ãƒ–ãƒ©ãƒ³ãƒé¸æŠãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯

dev_add_building_name ãƒ–ãƒ©ãƒ³ãƒã«åˆ‡ã‚Šæ›¿ãˆã¾ã™

3. ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ
GitHubã¯ãƒ–ãƒ©ãƒ³ãƒã‚’åˆ‡ã‚Šæ›¿ãˆãŸçŠ¶æ…‹ã§ ã€ŒCompare & pull requestã€ ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã—ã¦ãã‚Œã¾ã™

å‡ºãªã„å ´åˆã¯ä¸Šãƒ¡ãƒ‹ãƒ¥ãƒ¼ã® Pull requests â†’ New pull request ã‚’ã‚¯ãƒªãƒƒã‚¯

Baseï¼ˆçµ±åˆå…ˆï¼‰ã‚’ devã€Compareï¼ˆçµ±åˆå…ƒï¼‰ã‚’ dev_add_building_name ã«è¨­å®š

4. å·®åˆ†ã‚’ç¢ºèª
ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚¿ãƒ–ã§ã€storage.py ãªã©ã®å¤‰æ›´ç‚¹ã« ã€Œç‰©ä»¶åã€è¿½åŠ ã®å·®åˆ†ãŒã‚ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯

å•é¡Œãªã‘ã‚Œã°å³ä¸Šã®ç·‘ãƒœã‚¿ãƒ³ ã€ŒCreate pull requestã€ ã‚’æŠ¼ã—ã¾ã™

5. PRã®ä½œæˆã¨ãƒãƒ¼ã‚¸
ã‚¿ã‚¤ãƒˆãƒ«ä¾‹: Add property_name (building name) support into dev

å†…å®¹: ã€Œsuumo_mansion_review.py, storage.py, normalize.py ã®ä¿®æ­£ã§ç‰©ä»¶åã‚’CSVã¾ã§åæ˜ ã€

ä½œæˆã—ãŸã‚‰ PR ç”»é¢ã«ç§»ã‚Šã¾ã™

ã€ŒMerge pull requestã€ â†’ ã€ŒConfirm mergeã€ ã‚’ã‚¯ãƒªãƒƒã‚¯

6. çµ±åˆå®Œäº†ã®ç¢ºèª
dev ãƒ–ãƒ©ãƒ³ãƒã«åˆ‡ã‚Šæ›¿ãˆã¦ã‚³ãƒ¼ãƒ‰ã‚’è¦‹ã‚‹ã¨ã€storage.py ã« property_name ãŒå…¥ã£ã¦ã„ã‚‹ã¯ãšã§ã™

å¿…è¦ãªã‚‰ chatgpt_log/ ã«ã€Œ202509xx_building_nameçµ±åˆã€ãªã©ãƒ¡ãƒ¢ã‚’è¿½åŠ ã—ã¦ãŠãã¨å±¥æ­´ãŒæ˜ç¢ºã«ãªã‚Šã¾ã™

ğŸ’¡ ãƒã‚¤ãƒ³ãƒˆ
ã‚¦ã‚§ãƒ–æ“ä½œã ã‘ã§ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼â†’ãƒãƒ¼ã‚¸ã€ã¾ã§å¯èƒ½ã§ã™

ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆãŒå‡ºãŸå ´åˆã¯ GitHub ä¸Šã§ã‚‚è§£æ¶ˆã§ãã¾ã™ï¼ˆã‚¨ãƒ‡ã‚£ã‚¿ç”»é¢ãŒé–‹ãã¾ã™ï¼‰

ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ä½œç¢ºèªã§ããªã„ãªã‚‰ã€GitHub Actions ã‚„ Codespaces ã‚’ä½¿ã†æ–¹æ³•ã‚‚ã‚ã‚Šã¾ã™ï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ãƒ†ã‚¹ãƒˆã§ãã‚‹ï¼‰

ğŸ‘‰ ç¥å´ã•ã‚“ã¯ã€Œã™ãã« dev ã«ãƒãƒ¼ã‚¸ã€ã—ãŸã„ã§ã™ã‹ï¼Ÿ
ãã‚Œã¨ã‚‚ã€ŒPR ã‚’ä½œã£ã¦ã‹ã‚‰ã€å†…å®¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ â†’ ãƒãƒ¼ã‚¸ã€ã™ã‚‹æµã‚Œã‚’ä¸€ç·’ã«ç¢ºèªã—ã¾ã™ã‹ï¼Ÿ

ã‚ãªãŸ:
ã“ã¡ã‚‰ãŒã€ãƒãƒ£ãƒƒãƒˆæœ€çµ‚ã‚³ãƒ¡ãƒ³ãƒˆã¨ãªã‚Šã¾ã™ã€‚
ãƒãƒ£ãƒƒãƒˆå†…å®¹ã‚’ãƒŠãƒ¬ãƒƒã‚¸ã«è¿½åŠ ã—ã¾ã™ã€‚
ChatGPT:
æ‰¿çŸ¥ã—ã¾ã—ãŸ âœ…
ä»Šå›ã®ãƒãƒ£ãƒƒãƒˆã®å†…å®¹ã‚’ãƒŠãƒ¬ãƒƒã‚¸ã«è¿½åŠ ã™ã‚‹ãªã‚‰ã€ã¾ã¨ã‚ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã™ã‚‹ã¨æ•´ç†ã—ã‚„ã™ã„ã¨æ€ã„ã¾ã™ã€‚

ğŸ“˜ ãƒŠãƒ¬ãƒƒã‚¸è¿½åŠ ç”¨ã¾ã¨ã‚
1. LLMé–‹ç™ºã®ç†è§£
ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã‚’æ§‹ç¯‰ã™ã‚‹ã®ã§ã¯ãªãã€æ—¢å­˜ã® LLMï¼ˆGPTãªã©ï¼‰ã‚’åˆ©ç”¨ã™ã‚‹ã€‚

ã‚ªãƒ¼ãƒ—ãƒ³ãƒ–ãƒƒã‚¯è©¦é¨“æ–¹å¼ã§ã€è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿ï¼ˆRAGï¼‰ã‚„å¤–éƒ¨APIã‚’å‚ç…§ã•ã›ã‚‹ã®ãŒä¸­å¿ƒã€‚

2. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè£…
suumo_mansion_review.py ã‚’æ”¹ä¿®ã—ã¦ ç‰©ä»¶åã‚‚å–å¾—ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã€‚

cassetteå‹ DOM ã¨ property_unitå‹ DOM ã®ä¸¡æ–¹ã«å¯¾å¿œã€‚

ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆpn=ï¼‰ã‚„ pc=100 ã«ã‚‚å¯¾å¿œã€‚

3. CSVä¿å­˜ã®ä¿®æ­£
storage.py ã‚’ä¿®æ­£ã—ã€property_nameï¼ˆç‰©ä»¶åï¼‰ ã‚’æ­£å¼ã‚«ãƒ©ãƒ ã¨ã—ã¦è¿½åŠ ã€‚

metaâ†’å†…éƒ¨åˆ—ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ title ã‚’ç‰©ä»¶åã«æ¡ç”¨ã€‚

å‡ºåŠ›ã¯ BOM ä»˜ãUTF-8ã€æ¯å›æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«åã€‚

4. é–‹ç™ºãƒ•ãƒ­ãƒ¼
dev_add_building_name ãƒ–ãƒ©ãƒ³ãƒã§å‹•ä½œç¢ºèª â†’ dev ã¸ãƒãƒ¼ã‚¸äºˆå®šã€‚

GitHubã‚¦ã‚§ãƒ–UIã ã‘ã§ Pull Request ã‚’ä½œæˆãƒ»ãƒãƒ¼ã‚¸å¯èƒ½ã€‚

5. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒªã‚¹ã‚¯
ChatGPTã«æš—å·åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥è§£å‡ã•ã›ã‚‹ã“ã¨ã¯ã§ããªã„ã€‚

å®Ÿé‹ç”¨ã§ã¯ã€Œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§æš—å·åŒ–/å¾©å· â†’ GPTã«ã¯å¹³æ–‡ã ã‘æ¸¡ã™ã€ã€‚

B2Bé¡§å®¢å‘ã‘ã«ã¯ã€ŒAPIåˆ©ç”¨ãƒ‡ãƒ¼ã‚¿ã¯å­¦ç¿’ã«ä½¿ã‚ã‚Œãªã„ã€ã€Œã‚ªãƒ³ãƒ—ãƒ¬LLMã‚‚å¯èƒ½ã€ã¨èª¬æ˜ã™ã‚‹ã€‚

ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯ åˆ©ç”¨è¦ç´„ãƒ»robots.txt ã®ç¢ºèªã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ãƒ»IPãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒ­ã‚°ç®¡ç†ãŒå¿…é ˆã€‚VPNåˆ©ç”¨è‡ªä½“ã¯åˆæ³•ã ãŒã€è¦ç´„é•åã®ã‚«ãƒãƒ¼ã«ã¯ãªã‚‰ãªã„ã€‚

ğŸ‘‰ ã“ã®å½¢ã§æ®‹ã™ã¨ã€ŒLLMã®ç†è§£ã€ã€Œå®Ÿè£…ãƒã‚¦ãƒã‚¦ã€ã€Œã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»æ³•å‹™ã€ã€ŒGitHubé‹ç”¨ã€ã®4æœ¬æŸ±ã§æ•´ç†ã§ãã¾ã™ã€‚

ç¥å´ã•ã‚“ã€ã“ã®ã¾ã¾ç§ã®ã»ã†ã§ è¦ç´„ç‰ˆã‚’ãƒŠãƒ¬ãƒƒã‚¸å½¢å¼ï¼ˆMarkdownï¼‰ ã«æ•´ãˆã¦ãŠæ¸¡ã—ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ




é¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“é¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“
ChatGPT ã®å›ç­”ã¯å¿…ãšã—ã‚‚æ­£ã—ã„ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚é‡è¦ãªæƒ…å ±ã¯ç¢ºèªã™ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
