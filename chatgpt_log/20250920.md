# Suumo スクレイピング & データクレンジング 開発ログ

## 1. CSV ロード & 前処理
- `address_normalize_functions.py` を作成
  - CSV を読み込み、必要カラムのみ抽出 (`TARGET_COLUMNS`)
  - 存在しないカラムは `None` で埋める
  - 全ファイルを結合して返す関数 `load_selected_columns`
- 正規化関数を作成
  - `normalize_location_numbers`: 全角数字を半角化
  - `normalize_exclusive_area`: 「㎡」を削除し float 化
  - `normalize_price`: 「万円」を削除し *10,000 して Int64 化  
    → `〇億〇〇〇万` パターンにも対応
    → `0円` は DROP
  - `normalize_station_walk`: 「徒歩」「分」を削除し int 化
  - `normalize_built_year`: 築年のみ抽出し int64 化
- `address_normalize.py` で `.pipe(...)` で連結して処理

## 2. warning 対応
- pandas の DeprecationWarning（pyarrow 必須化）の非表示処理を追加

## 3. layout (間取り) 分解
- `layout.unique()` でユニーク一覧確認
- LDK, ワンルーム, K, S, 納戸を新カラム化
  - LDK, K, R は数値列
  - 納戸は 1/0 フラグ
  - null は 0 fill
- CSV 出力して検証（5,005行でズレ検出 → 調整）

## 4. property_name 前処理
- 営業用記号や不要文字を削除
  - ◆, ◇, □, ■, ～, 【】, （）, ）
  - 「…」以降削除
  - 「2003年」など数字+年は削除
  - ♪, 万円, 可 も削除対象

## 5. location 前処理
- 東京都のとき、区・市の区切りで `ward_city` を抽出
- `city_town` も追加（例: 八王子市高尾町 → ward_city=八王子市, city_town=高尾町）
- 「町田」など誤検出を考慮 → 「町」では split しない

## 6. suumo スクレイパ追加
- 既存 `suumo_mansion_review.py` (中古マンション) に加え以下を追加
  - `suumo_mansion_review_ikkodate.py`（新築戸建て）
  - `suumo_mansion_review_shinchiku.py`（新築マンション）
  - `suumo_mansion_review_tochi.py`（土地）
- `pipeline_scrape_suumo.py` を統合更新
  - URL パターンでスクレイパを切替
    - `/ikkodate/` → 戸建て
    - `/ms/shinchiku/` → 新築マンション
    - `/tochi/` or `bs=030` → 土地
    - `/ms/chuko/` → 中古マンション
  - `is_list_page` をルールごとに判定

## 7. 物件タイプ別の処理ロジック
### 戸建て (ikkodate)
- 価格、所在地、徒歩分数、土地面積、建物面積、間取りを取得
- 面積は「xx.xx㎡」に統一、最初の数字のみ取得
- 「7980万～8480万」「8480万・8780万」はクレンジングで処理（スクレイプ段階ではノータッチ）

### 新築マンション (shinchiku)
- `.cassette-result_detail` ごとに複数物件を抽出
- 間取り・面積 → `2LDK・3LDK / 48.82m2～70.58m2` → 最初の間取り・最初の面積のみ
- 価格 → 「1億600万円～1億8000万円」 → 最初の数字のみ円換算
- 引渡時期も meta に追加

### 土地 (tochi)
- 項目例
  - 所在地
  - 販売価格
  - 駅徒歩
  - 土地面積（xx.xx㎡）
  - 坪単価
  - 建ぺい率・容積率
- 間取りは空欄で OK
- CSV では `exclusive_area_sqm` にも土地面積を複写して保存されるように修正
- `parse_detail_page` でも土地面積が取れるよう `_pick_value_by_label` を実装

## 8. デバッグ・テスト
- `suumo_mansion_review_tochi_test.py` を作成
  - 特定 URL を fetch → `meta` を直接 print
  - テスト結果では `土地面積=41.77㎡` が正常に取得できた
- ただし `pipeline_scrape_suumo.py` 経由の CSV 出力では `exclusive_area_sqm` が空
  - 理由: list parser 側で `exclusive_area_sqm` を "" のままにしていた
  - 修正: `exclusive_area_sqm` と `専有面積` に `土地面積` を複写して保存するよう修正
  - storage 側のマッピングでも fallback 追加を推奨

## 9. 未解決・今後
- 「敷地面積」表記に対応するため `_pick_value_by_label` に別名を追加する予定
- 価格の表記ゆれ（億、万、～、・）はスクレイピングでは保持、クレンジングで解決
- pipeline 側 CSV で `exclusive_area_sqm` が正しく保存されるか再検証必要


##午後追加

# 不動産スクレイピング & 分析プロジェクトまとめ

## 1. データ収集とスクレイピング
- **対象サイト**: SUUMO
- **対象種別**:
  - 戸建て（中古、新築）
  - マンション（中古、新築）
  - 土地
- **起点**: 中古戸建て・中古マンション
- **予測対象**: マンション（中古/新築）、土地
- **課題**:
  - `chukoikkodate` で面積が取れない問題を解決 → `scraper/suumo_mansion_review_ikkodate.py` を新規作成しロジック拡張

## 2. データクレンジング
### address_normalize_functions.py
- **標準化処理**:
  - 物件名クレンジング（記号削除・年号削除・空白整形）
  - 所在地 → `ward_city`, `city_town` 分離
  - 面積：㎡ に統一、数値化
  - 価格：億/万表記を数値化 (`price_jpy`)
  - 駅徒歩：`徒歩◯分` → 数値化
  - 築年：`built_ym` → 西暦4桁抽出
  - 間取り：`LDK, K, S, 1R, 納戸` の数値化

### address_normalize.py
- 各フォルダ内（`kodate_model_chuko`, `mansion_model_shin` 等）の csv をまとめて処理
- **最終出力**: `normalized_listings.csv`

## 3. 住所の数値化（ジオコーディング）
- `coordinates.csv` を使用し、`ward_city + city_town` 単位で緯度経度を集約
- 丁目表記は削除（例: `麹町六丁目 → 麹町`、`六番町` はそのまま）
- 小数点第2位（約1.1km精度）に丸め
- **出力列**: `x_min, x_max, y_min, y_max`

## 4. region_type の付与
- **定義**:
  - 1: 都心部（中央区、港区、新宿区、渋谷区、文京区、目黒区）
  - 2: その他23区＋三鷹市・武蔵野市
  - 3: 多摩エリア（立川市、府中市、国分寺市、日野市、八王子市）
- `ward_city → region_type` のマッピング辞書を作成し、新カラム付与

## 5. ラベル付与（y1, y2）
### y1: 中古 vs 土地
- `tochi` の平均価格（city_town→なければ ward_city）を基準
- 判定式:  
  `ratio = tochi_price / chuko_price`  
  `y1 = 1 if ratio > 1.5 else 0`  
  （参照がなければ NaN）

### y2: 中古 vs 新築
- 中古マンション → 新築マンション平均  
- 中古戸建 → 新築戸建平均  
- 判定式:  
  `ratio2 = shin_price / chuko_price`  
  `y2 = 1 if ratio2 >= 2 else 0`  
  （参照がなければ NaN）

- **最終出力**: `normalized_listings_with_y.csv`

## 6. 重複削除
- 物件ごとに一意化:
  - URL があれば `drop_duplicates(["url"])`
  - 無ければ `["property_name","ward_city","city_town","source_group","price_jpy"]` をキーに削除

## 7. 特徴量選択（寄与度分析）
- **対象特徴量**:
  - station_walk
  - built_ym
  - exclusive_area_sqm
  - layout（LDK, 1R, K, S, 納戸）
  - price_jpy
  - region_type
  - x_min, x_max, y_min, y_max

### ジニ係数（決定木ベース）
- 上位寄与度:
  - x_max (0.41)
  - exclusive_area_sqm (0.38)
  - built_ym (0.14)
  - region_type (0.04)

### p値（ロジスティック回帰）
- built_ym（p ≈ 1e-47）→ 極めて有意
- region_type（p ≈ 1e-26）→ 有意
- exclusive_area_sqm（p ≈ 0.076）→ 弱いが有意水準10%程度
- x_max（p ≈ 0.26）→ 有意性なしだが寄与度は大きい

### 判断
- **採用すべき特徴量**: built_ym, region_type, exclusive_area_sqm, x_max  
- 実務的には、築年数＋地域特性＋面積＋地理的東西位置が建替え余地(y1)に寄与している

---

## 📌 まとめ
- **スクレイピング**: 種別ごとの専用パーサーを実装  
- **正規化**: 価格・面積・築年・間取り・所在地を統一形式に変換  
- **特徴量**: 地理情報（region_type, bbox）を数値化  
- **ラベル付与**:
  - y1 = 土地基準の建替え余地
  - y2 = 新築基準の建替え余地
- **特徴量選択**: built_ym・region_type・exclusive_area_sqm・x_max が主要因

このナレッジを基に、次ステップは **精度検証（ROC-AUC, F1 など）** や **説明可能性（SHAP, LIME）** の導入が可能。

## 夕方修正

# キャッチアップ要約（reconstruction_LLM / 2025-09-20時点）

## 0) リポジトリ / 対象
- Repo: `https://github.com/britishbluecat/reconstruction_LLM`（作業ブランチ: `dev`）
- 本スレ対象ディレクトリ: `src/`
- 主要データ: `normalized_listings_with_y.csv`
- 生成アーティファクト:
  - `models/best_model_y1.joblib`
  - `MODEL_METADATA_y1.json`
  - `pred_payload_example.csv`

---

## 1) スクレイピング〜前処理の要点（抜粋）
- SUUMOスクレイパー改修：物件名取得、DOM差異対応、`pn=` ページネーション、`pc=100`、種別拡張（中古/新築/土地）。
- 正規化：
  - 価格・面積・徒歩分・築年の単位統一（億/万→円、分→int、YYYYMM→int 等）
  - `layout` 分解（LDK / 1R / K / S / nando）
  - 土地の場合は面積を `exclusive_area_sqm` にも複写保存
- 重複判定キー：`url` 優先、無い場合は `["property_name", "ward_city", "city_town", "source_group", "price_jpy"]`

---

## 2) 特徴量重要度の調査（gini_importance.py）
- 対象特徴量（13列）:
  `["station_walk","built_ym","exclusive_area_sqm","LDK","1R","K","S","nando","region_type","x_min","x_max","y_min","y_max"]`
- 決定木(Gini)×ロジット(p値)の併用で、特に寄与が高いのは：
  - `exclusive_area_sqm`, `built_ym`, `x_max`,（続いて `x_min`, `station_walk`, `region_type` など）

---

## 3) 学習スクリプト（train_models相当）の実行と結果
- モデル候補：`LogisticRegression`, `RandomForest`, `HistGradientBoostingClassifier`
- 評価：Stratified 5-fold CV (scoring=ROC-AUC) → テストで再評価
- しきい値：`precision_recall_curve` から **F1最大化閾値** を自動決定
- 実測（HGBの例）：
  - `test_roc_auc`: **0.8715**
  - `best_threshold`: **0.34136**
  - `accuracy@opt`: **0.8107**
  - `F1@opt`: **0.8577**
  - `confusion_matrix@opt`: `[[256, 154], [48, 609]]`
  - Feature Importances（上位抜粋）: `exclusive_area_sqm, built_ym, x_max, x_min, station_walk, LDK, region_type,...`
- 保存：`models/best_model_y1.joblib` に **パイプライン＋閾値＋特徴量** をバンドル

---

## 4) 推論アーティファクトの標準化
- **MODEL_METADATA_y1.json**：
  - `features`, `threshold`, `cv_best_params`, OOFメトリクス、環境バージョン等
- **pred_payload_example.csv**：
  - 特徴量13列のダミー1行（中央値ベース）を出力
- 目的：RAG/外部ツールから **入出力契約（I/O）を可視化** し、`.joblib` だけに依存しない運用

---

## 5) Excel GUI 連携（excel_gui_v2.py）
### 5.1 主な設計
- 事前学習モデルを **再学習せずに読み込む** 方式へ
  - `MODEL_BUNDLE_PATH = "models/best_model_y1.joblib"`
  - `MODEL_METADATA_PATH = "MODEL_METADATA_y1.json"`
- Excel入力（住所/築年/面積 等）→ **ML特徴量13列** に成形 → `predict_proba` で `y1` 確率を算出
- LLM（`gpt-4o-mini`）に **3案+解体まとめ** をJSONで生成させて Excel に整形出力

### 5.2 追加の出力列（可視化）
- `モデル根拠TOP1`, `モデル根拠TOP2`, `モデル根拠TOP3`
  - 1行ごとの **局所説明**（SHAPがあればSHAP寄与、なければ「重要度×代表値差分」ヒューリスティック）
  - **挿入位置（forループ内）**：
    - 列名タイポ修正（`"解体の適否（暫定）」" → "解体の適否（暫定）"`）の **直後** に以下を追加
    ```python
    tops = signals.get("model_top", [])
    df.at[idx, "モデル根拠TOP1"] = tops[0] if len(tops) > 0 else ""
    df.at[idx, "モデル根拠TOP2"] = tops[1] if len(tops) > 1 else ""
    df.at[idx, "モデル根拠TOP3"] = tops[2] if len(tops) > 2 else ""
    ```
- 既知のバグ修正：
  - `built_ym = int(f"{built_year}01}")` → **余計な `}` を除去**。安全に `int(built_year)*100+1` へ置換済み

---

## 6) 例外対応・品質ガード
- `y1` の NaN/∞ を検出 → **無効行を除外**、`rows_dropped_missing_or_invalid_target.csv` へ出力
- LLM出力は **JSONパースガード**（コードフェンス除去→`json.loads`→構造検証）
- 任意：`jsonschema` を導入し、`plans/demolition_summary` のスキーマ検証と再試行を追加可能

---

## 7) CLI一括推論（RAG前段の前処理）
- `infer_joblib.py`（提案済み）：
  - 入力：`normalized_listings_with_y.csv`
  - 出力：`predictions_y1.csv`（`y1_proba`, `y1_pred` を付与）

---

## 8) ネットワーキング用・3点要約（エレベーターピッチ）
1. **何を作っているか**：空き物件の意思決定を支援する **「データ×LLMアドバイザー」**。Excelを選ぶだけで **建て替え／リノベ／解体** の3案を自動提示。
2. **どう動くか**：スクレイピング→正規化→**学習済みモデル（AUC≈0.87）** で確率・スコア→LLMが **3案＋リスク＋根拠** をJSONで出力→Excel整形。RAGで相場メモ等も参照。
3. **導入メリット**：提案時間の短縮、**根拠の見える化** による説明容易性、`.joblib＋メタJSON` で **安定運用/差し替え** が簡単。

---

## 9) 実行コマンド例

# venv を有効化後
python -m gini_importance     # モデル学習＆joblib/メタ/ペイロード生成（recalculate=True時）
python -m excel_gui_v2        # GUI起動（.env の OPENAI_API_KEY が必要）
python infer_joblib.py        # 一括推論 → predictions_y1.csv

## 10) 既知の課題 / TODO
- [ ] `MODEL_METADATA_y1.json` の `training_data` に `n_samples_total / used` を記録（既に提案済み）
- [ ] `region_type` の推定ロジック高度化（23区/政令市/郊外など）
- [ ] SHAPの安定化（`shap` 導入＆環境整備）：`pip install shap`
- [ ] LLM出力の **JSON Schema** バリデーション＋1回リトライ
- [ ] RAG最小構成：`property_name, ward_city, city_town, price_jpy, exclusive_area_sqm` で相場スニペット検索
- [ ] 予測ログ（入力→確率→最終判定→LLM JSON）をCSV保存し、継続的にモデル監視

## 11) 環境メモ
- `.env` に `OPENAI_API_KEY=...`
- 主要ライブラリ：`pandas`, `numpy`, `scikit-learn`, `joblib`, `statsmodels`, （任意）`shap`
- Windows実行時のパス例：`C:\Users\user\Documents\python\reconstruction_LLM\src\...`

